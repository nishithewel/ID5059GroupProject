{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905544f4",
   "metadata": {},
   "source": [
    "# Assuming we have our data cleaned and our own models run with test dataset, the following codes only plot ROC curve and calculate AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67817187",
   "metadata": {},
   "source": [
    "# Generating ROC Curve (my model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51f49a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_curve, roc_auc_score\n",
    "\n",
    "def roc(myModel,testData):\n",
    "    model_pred = myModel.decision_function(testData) #Why having this?\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        testData,model_pred)\n",
    "    plt.show()\n",
    "    return model_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c261620",
   "metadata": {},
   "source": [
    "# Prediction Probabilities (my model & random prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6eaeeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_probs(myModel, testData):\n",
    "    random_probs = [0 for _ in range(len(testData))]\n",
    "    model_probs = myModel.predict.proba(testData)\n",
    "    model_probs = model_probs[:,1]\n",
    "    return random_probs, model_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7e37f",
   "metadata": {},
   "source": [
    "# Calculate AUC (compare with random prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6de2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(testData, myProbs, modelPred, randomProbs):\n",
    "    random_auc = roc_auc_score(testData, randomProbs)\n",
    "    model_auc = roc_auc_score(testData, myProbs)\n",
    "    model_pred_auc = roc_auc_score(testData, modelPred) #Why having this?\n",
    "    print('Random (chance) Prediction: AUC = %.3f' % (random_auc))\n",
    "    print('My Model: AUC = %.3f' % (model_auc))\n",
    "    print('Model_pred: AUC = %.3f' % (modelPred)) #Why having this?\n",
    "    return random_auc, model_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfea520",
   "metadata": {},
   "source": [
    "# Calculate ROC Curve (my model and random prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79ff4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_roc(testData, myProbs, randomProbs):\n",
    "    my_fpr, my_tpr, _ = roc_curve(testData, myProbs)\n",
    "    random_fpr, random_tpr, _ = roc_curve(testData, randomProbs)\n",
    "    return my_fpr, my_tpr, random_fpr, random_tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1724ebd",
   "metadata": {},
   "source": [
    "# Plot the ROC Curve (compare with random prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5381f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(myFPR, myTPR, myAUC, randomFPR, randomTPR, randomAUC):\n",
    "    plt.plot(myFPR, myTPR, marker = '-', label = 'My model (AUC = %0.3f)' % myAUC)\n",
    "    plt.plot(randomFPR, randomTPR, lineStyle = '--', label = 'Random Prediction (AUC = %0.3f)' % randomAUC)\n",
    "    \n",
    "    #Title\n",
    "    plt.title(\"ROC Plot\")\n",
    "    #Axis Labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    #Show Legend\n",
    "    plt.legend()\n",
    "    #Show Plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
