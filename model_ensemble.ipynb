{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fitted several models, we now combine them to try squeeze out any further predictive performance. This is achieved through the use of SKLearn's  `StackingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'xgb':xgb_cl,\n",
    "          'bag': bag_clf,\n",
    "          'lgb': lgb_clf,\n",
    "          'rf':rf_final,\n",
    "          'AdaBoost': adaModel}\n",
    "stacked_model = stack_models(*models.items(), cv_folds=3)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "models['stacked'] = stacked_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to show the performance of each model\n",
    "performance = pd.DataFrame(y_test)\n",
    "performance['stacking_prediction'] = stacked_model.predict(X_test)\n",
    "for m in stacked_model.named_estimators_:\n",
    "        performance[m] = stacked_model.named_estimators_[m].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how each of our models correlate with our target\n",
    "print(\"Correlations with target column\")\n",
    "print(temp.corr()['isFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "# evaluate models using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, m in models.items():\n",
    "\tscores = evaluate_model(m, X_test, y_test)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint(f'>{name} {mean(scores):0.3f} ({std(scores):0.3f})')\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.diagnostics import get_diagnostics\n",
    "get_diagnostics(*models.items(), y_true=y_test, X=X_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
