{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishithewel/ID5059GroupProject/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nishithewel/ID5059GroupProject.git"
      ],
      "metadata": {
        "id": "PaV8oqnO6kQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c4463f-a4f0-4a2a-d9c1-b12ec2537a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ID5059GroupProject'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (215/215), done.\u001b[K\n",
            "remote: Total 265 (delta 144), reused 107 (delta 49), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (265/265), 14.63 MiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os \n",
        "# os.chdir('/content/ID5059GroupProject')\n",
        "# ! git pull"
      ],
      "metadata": {
        "id": "oDruvEAwl-X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the Data from kaggle."
      ],
      "metadata": {
        "id": "29l0EzjK5k2b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cR9JURfP6Gs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install -q kaggle\n",
        "# from google.colab import files\n",
        "# files.upload()\n",
        "# !rm -rf sample_data\n",
        "# !mkdir  -p /root/.kaggle/\n",
        "# !mv kaggle.json  /root/.kaggle/\n",
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "# !ls -lart /root/.kaggle/\n",
        "# !kaggle competitions download -c 'ieee-fraud-detection'\n",
        "# !mkdir data\n",
        "# !mv ieee-fraud-detection.zip data/\n",
        "# !unzip data/ieee-fraud-detection.zip -d data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPqFgUOESMBt"
      },
      "source": [
        "## Introduction to the Dataset\n",
        "\n",
        " - Vivesh "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmzwh08RSizC"
      },
      "source": [
        "### Team Members\n",
        "- Nishithe Welandawe - naw4\n",
        "- Joseph Edwards - jde1\n",
        "- Anli Hu - 373\n",
        "- Yao-Ting Wang - ytw1\n",
        "-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1lBgJxYSZNQ"
      },
      "source": [
        "### Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLCJlNf8Hq4G",
        "outputId": "7e61fe83-3e4b-4779-d866-ce16f02af5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install imblearn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5enJhawlOH1u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "os.chdir('/content/ID5059GroupProject')\n",
        "from imblearn import over_sampling\n",
        "from utils.preprocess import preprocess\n",
        "from utils.reduce_memory import reduce_mem_usage\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils.model_stack import stack_models\n",
        "from random import sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JitrODFSs8N"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "- Download the dataset from Kaggle and save it the GDrive session.\n",
        "- Due to the large size, to keep loading size down we optimize data types to reduce memory usage.\n",
        "- Merge the Transaction and Identity data set to create the 'Test' set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYkvgdPIS_HF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930dca32-5073-496d-9606-897fb801d994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Memory usage of dataframe is 1519.24 MB\n",
            "Memory usage after optimization is: 425.24 MB\n",
            "Decreased by 72.0%\n",
            "Memory usage of dataframe is 1775.15 MB\n"
          ]
        }
      ],
      "source": [
        "### You need to change these codes to load your dataset\n",
        "#if youre files are stored in google drive do this\n",
        "DRIVE_STORE = True\n",
        "DEBUG = True\n",
        "if DRIVE_STORE == True:\n",
        "  drive.mount('/content/drive') \n",
        "  root_dir = '/content/drive/MyDrive/Python/ID5059-GroupProject/ieee-fraud-detection/' #for nish\n",
        "  \n",
        "else:\n",
        "  root_dir = '/content/data/'\n",
        "os.chdir(root_dir)\n",
        "\n",
        "if DEBUG:\n",
        "  # test_iden = pd.read_csv(root_dir + 'test_identity.csv',nrows = 100)\n",
        "  # test_tran = pd.read_csv(root_dir +'test_transaction.csv',nrows = 100)\n",
        "  # train_iden = pd.read_csv(root_dir +'train_identity.csv',nrows = 100)\n",
        "  # train_tran = pd.read_csv(root_dir +'train_transaction.csv',nrows = 100)\n",
        "  test_iden = pd.read_csv(root_dir + 'test_identity.csv',)\n",
        "  test_tran = pd.read_csv(root_dir +'test_transaction.csv',)\n",
        "  train_iden = pd.read_csv(root_dir +'train_identity.csv',)\n",
        "  train_tran = pd.read_csv(root_dir +'train_transaction.csv',)\n",
        "\n",
        "test_tran = reduce_mem_usage(test_tran)\n",
        "train_tran = reduce_mem_usage(train_tran)  \n",
        "\n",
        "train = pd.merge(train_tran, train_iden, on = 'TransactionID', how = 'left')\n",
        "test = pd.merge(test_tran, test_iden, on = 'TransactionID', how = 'left')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ind = sample(range(len(train)),\n",
        "                        int(0.8*len(train))\n",
        "                        )\n",
        "#but we need a local test to run diagnostics as well\n",
        "local_test = train.iloc[test_ind]\n",
        "#drop rows from train\n",
        "train = train.drop(test_ind,axis = 0)\n",
        "\n",
        "del train_tran, train_iden, test_iden, test_tran"
      ],
      "metadata": {
        "id": "Jfth8_nJiVAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test_dataset and train_dataset had a mismatch in the name of id features. In the train_dataset id features were present with the name id_x where x was a value between 01 and 38 whereas in the test_dataset id features were of the form id-x. So, we changed the format of id features in the test_dataset from id-x to id_x."
      ],
      "metadata": {
        "id": "5k7iNoPKH90W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test.columns  = [col.replace('-','_') for col in test.columns]"
      ],
      "metadata": {
        "id": "3KXOFGpIFy6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkstYfcPSxty"
      },
      "source": [
        "## EDA\n",
        "We perform basic analysis on the data.\n",
        "- Visualise predictor distribution.\n",
        "- Correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLn6uqvSOc8C"
      },
      "outputs": [],
      "source": [
        "train['isFraud'].value_counts(normalize = True).plot(kind= 'barh')\n",
        "plt.title('% of class')\n",
        "plt.ylabel('isFraud (1 = Fraud)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW8YV043Pt05"
      },
      "source": [
        "We find very few instance of Fraud. Consider Class Balancing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9YkTe92VOQo"
      },
      "source": [
        "## Feature Preproccessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW1r2t5iAtg4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWxQjGBV7LlZ"
      },
      "source": [
        "Systematically, deal with features by considering the following,\n",
        "- 'M' columns - look like true or false lets convert them to 1, 0\n",
        "- 'D' columns - these are fine all numerical\n",
        "- 'C' columns - numerical except for 4,6\n",
        "\n",
        "We deal with categorical variables using dummy vars."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_0 = train\n",
        "test_0 = test\n",
        "local_test_0 = local_test"
      ],
      "metadata": {
        "id": "sPk_PMTo3-ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = train_0\n",
        "# test = test_0\n",
        "# local_test = local_test_0"
      ],
      "metadata": {
        "id": "GxCe2px_MJ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "BYA4XeSX4KaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXiN7Plz7K4x"
      },
      "outputs": [],
      "source": [
        "train = preprocess(train)\n",
        "test = preprocess(test)\n",
        "local_test = preprocess(local_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiotT2FWVzQK"
      },
      "source": [
        "\n",
        "What features are we dropping and why?\n",
        "\n",
        "- TransactionID - Only a unique identifier for identity\n",
        "- Id Columns - All Id columns contain at least 75% NaN values \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_cols = [col for col in train.columns if col.startswith('id')]\n",
        "df_id = train[id_cols]\n",
        "# df_id.head()\n",
        "nulls = df_id.isnull().sum()/len(df_id) #as a percentage\n",
        "nulls.sort_values(ascending=False).plot.bar(title='NaN as a %')\n",
        "# train.drop(id_cols, axis = 1,inplace = True)"
      ],
      "metadata": {
        "id": "TuaDw4E8Feg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this otherwise might cause ram overflows\n",
        "v_cols = [col for col in train.columns if col.startswith('V')]\n",
        "# # lets ignore the v columns for the time being\n",
        "train.drop(v_cols,axis =1, inplace = True)\n",
        "train.drop(id_cols, axis = 1,inplace = True)\n",
        "test.drop(v_cols,axis =1, inplace = True)\n",
        "test.drop(id_cols, axis = 1,inplace = True)\n",
        "local_test.drop(v_cols,axis =1, inplace = True)\n",
        "local_test.drop(id_cols, axis = 1,inplace = True)"
      ],
      "metadata": {
        "id": "21OQ3_nBg7AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vRkm6AdIm1Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = list(set(train.columns) - set(train._get_numeric_data().columns))\n",
        "#we get dummy for some features only\n",
        "dummy_cols = [ col for col in categorical_features \n",
        "                    if len(train[col].unique()) < 15]\n",
        "large_cats = list(set(train.columns) - set(train._get_numeric_data()) - set(dummy_cols))"
      ],
      "metadata": {
        "id": "MJEHLPhNYJFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder  #the disco import \n",
        "\n",
        "def dummy_transform(df,encoder,col):\n",
        "  new_col = [col+'_' + str(i)  for i in encoder.categories_[0].tolist()]\n",
        "  dummy_arr = encoder.transform(df[col].to_numpy().reshape(-1, 1))\n",
        "  dummy_df = pd.DataFrame(dummy_arr, columns = new_col)\n",
        "\n",
        "  # Allows nice concat-ing\n",
        "  dummy_df.reset_index(drop=True, inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  return pd.concat([df, dummy_df],axis =1)\n",
        "\n",
        "for col in dummy_cols:\n",
        "  enc = OneHotEncoder(handle_unknown='ignore',sparse=False) \n",
        "  dummy_train = enc.fit_transform(train[col].to_numpy().reshape(-1, 1))\n",
        "  \n",
        "  train = dummy_transform(train,enc,col)\n",
        "  local_test = dummy_transform(local_test,enc,col)\n",
        "  test = dummy_transform(test,enc,col)\n",
        "  print('encoding',col)\n",
        "  # # for df in [train, test]:\n",
        "  # dummy_test = enc.transform(test[col].to_numpy().reshape(-1, 1))\n",
        "\n",
        "  # dummy_dftest = pd.DataFrame(dummy_test, columns = new_col)\n",
        "  # dummy_dftrain = pd.DataFrame(dummy_train, columns = new_col)\n",
        "\n",
        "  # test = pd.concat([test, dummy_dftest],axis =1)\n",
        "  # train = pd.concat([train, dummy_dftrain],axis =1)\n",
        "\n",
        "    # df.drop(col,axis =1,inplace=True)\n",
        "train.drop(dummy_cols,axis =1,inplace = True )\n",
        "test.drop(dummy_cols,axis =1,inplace = True )\n",
        "local_test.drop(dummy_cols,axis =1,inplace = True )\n",
        "# train.columns"
      ],
      "metadata": {
        "id": "TGH6katghs3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['isFraud'].unique()"
      ],
      "metadata": {
        "id": "dN1HSUsOQQGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "ovYizFKXMkmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to deal with the variables with high number of cats."
      ],
      "metadata": {
        "id": "xee7uX0rkLZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_encoder(x,groupby):\n",
        "  if x == None:\n",
        "    return groupby.get(np.nan)\n",
        "  else:\n",
        "    return groupby.get(x)\n",
        "\n",
        "\n",
        "  # large_cats\n",
        "for col in large_cats:\n",
        "  freq_enc = (train.groupby(col,dropna=False  # to prevent loss of information\n",
        "                            ).size()) / len(train)\n",
        "  train[col+'freq'] = train[col].apply(lambda x : frequency_encoder(x,freq_enc))\n",
        "  test[col+'freq'] = train[col].apply(lambda x : frequency_encoder(x,freq_enc))\n",
        "  local_test[col+'freq'] = train[col].apply(lambda x : frequency_encoder(x,freq_enc))\n",
        "\n",
        "train.drop(large_cats,axis = 1,inplace = True)\n",
        "test.drop(large_cats,axis = 1,inplace = True)\n",
        "local_test.drop(large_cats,axis = 1,inplace = True)"
      ],
      "metadata": {
        "id": "FQP9TPz3dZ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcCP4HTGVU8c"
      },
      "source": [
        "### Column Imputation\n",
        "How are we dealing with NaN/Missing/Infinite Values?\n",
        "\n",
        "To use SMOTE we must remove np.infs and NaNs,\n",
        "- So we initially drop columns with less than 80% non-NA values.\n",
        "- Then, we drop rows containing NaN values.\n",
        "\n",
        "This using performance resulted in a higher performance on \n",
        "balanced score measures than keeping NaNs and not using SMOTE.\n",
        "\n",
        "We shoudl experiment with fillna\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_na = train.columns[train.isnull().sum()/len(train)>0.2]\n",
        "train_na = train.drop(high_na, axis=1)\n",
        "test = test.drop(high_na, axis=1)\n",
        "local_test = local_test.drop(high_na, axis=1)"
      ],
      "metadata": {
        "id": "nuUNmLH6dpfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0NVA0p4ZdPP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nulls = train_na.isnull().sum()/len(train_na) #as a percentage\n",
        "nulls.sort_values(ascending=False).plot.barh(title='NaN as a %')"
      ],
      "metadata": {
        "id": "zr2zbMLZbE10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAxxagslTJ6r"
      },
      "source": [
        "### Feauture Cleaning\n",
        "The data contains outliers, which require cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hlj3dq4VnZ3"
      },
      "source": [
        "### Feature engineering (Creation)\n",
        "Wrapped as Preprocessor function to be implemented after test train split."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G6-f0nEkOsxz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLJf4s37RJ-F"
      },
      "source": [
        "### Class Balancing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_no_na = train_na[~train_na.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "test = test[~local_test.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "local_test = local_test[~test.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "# train_no_na = train_no_na._get_numeric_data()\n",
        "X_train, y_train = train_no_na.drop('isFraud',axis = 1), train_no_na['isFraud']\n",
        "oversample = over_sampling.SMOTE()\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "X_train.drop('TransactionID', axis=1, inplace = True)\n",
        "X_test, y_test = local_test.drop(['isFraud', 'TransactionID'], axis = 1), local_test['isFraud']"
      ],
      "metadata": {
        "id": "094CmyxEKjqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts(normalize = True).plot(kind= 'barh')\n",
        "plt.title('% of class')\n",
        "plt.ylabel('isFraud (1 = Fraud)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hwv_2VUQrDB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXyRlGfW-DY"
      },
      "source": [
        "## Model Creation & Fitting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WZU6VvCTQyy"
      },
      "source": [
        "### Individiual models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
        "# model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)"
      ],
      "metadata": {
        "id": "r4WA3xbQN1pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix_bdnscBIGN"
      },
      "outputs": [],
      "source": [
        "#naw4\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
        "           'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "           'subsample': np.arange(0.5, 1.0, 0.1),\n",
        "           'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
        "           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
        "           'n_estimators': [100, 500, 1000]}\n",
        "\n",
        "# train.drop(['P_emaildomain', 'R_emaildomain'],axis =1 ,inplace = True)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_cl = xgb.XGBClassifier(\n",
        "    # tree_method='gpu_hist', gpu_id=0\n",
        "    )\n",
        "\n",
        "# clf = RandomizedSearchCV(estimator=xgb_cl,\n",
        "#                          param_distributions=params,\n",
        "#                          n_jobs = -1,\n",
        "#                         #  scoring='neg_mean_squared_error',\n",
        "#                          n_iter=25,\n",
        "#                          verbose=1)\n",
        "# search = clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "xgb_cl.fit(X_train, y_train)\n",
        "preds = xgb_cl.predict(X_test)\n",
        "accuracy_score(y_train, preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clf.get_params()"
      ],
      "metadata": {
        "id": "29fcUeiK6fUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bst.save_model('model_file_name.json')"
      ],
      "metadata": {
        "id": "BQac-z1Zt0lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wKBjfAhEfNz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_predictions(y_train, preds,normalize='true')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Qd-bjGfktV"
      },
      "outputs": [],
      "source": [
        "#jde1\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [\n",
        "  {'n_estimators': [10, 20, 50, 100, 500]}\n",
        " ]\n",
        "\n",
        "bag = BaggingClassifier(max_features=0.8, max_samples=0.8)\n",
        "\n",
        "scores = [\"precision\", \"roc_auc\"]\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    bag_clf = GridSearchCV(bag,\n",
        "                           param_grid,\n",
        "                           scoring=score,\n",
        "                           n_jobs = -1)\n",
        "    bag_clf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(bag_clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = bag_clf.cv_results_[\"mean_test_score\"]\n",
        "    stds = bag_clf.cv_results_[\"std_test_score\"]\n",
        "    for mean, std, params in zip(means, stds, bag_clf.cv_results_[\"params\"]):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBJYuF2MtIRI"
      },
      "outputs": [],
      "source": [
        "preds = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gm1JyaFo8Vg"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y_train, preds,normalize='true')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip uninstall lightgbm -y\n",
        "\n",
        "# install lightgbm GPU\n",
        "! pip install lightgbm --install-option=--gpu --install-option=\"--opencl-include-dir=/usr/local/cuda/include/\" --install-option=\"--opencl-library=/usr/local/cuda/lib64/libOpenCL.so\"\n"
      ],
      "metadata": {
        "id": "9nqv7c6NnUr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anli\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "params = {'num_leaves': 491,\n",
        "          'min_child_weight': 0.03454472573214212,\n",
        "          'feature_fraction': 0.3797454081646243,\n",
        "          'bagging_fraction': 0.4181193142567742,\n",
        "          'min_data_in_leaf': 106,\n",
        "          'objective': 'binary',\n",
        "          'max_depth': -1,\n",
        "          'learning_rate': 0.006883242363721497,\n",
        "          \"boosting_type\": \"gbdt\",\n",
        "          \"bagging_seed\": 11,\n",
        "          \"metric\": 'auc',\n",
        "          \"verbosity\": -1,\n",
        "          'reg_alpha': 0.3899927210061127,\n",
        "          'reg_lambda': 0.6485237330340494,\n",
        "          'random_state': 5313,\n",
        "         }\n",
        "from sklearn.model_selection import train_test_split\n",
        "score = 0\n",
        "\n",
        "feature_importances = pd.DataFrame()\n",
        "lgb_X_train, lgb_X_valid,lgb_y_train, lgb_y_valid=train_test_split(X_train,y_train,train_size=0.8)\n",
        "dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "dvalid = lgb.Dataset(X_test, label=y_test)\n",
        "lgb_clf = lgb.train(params, dtrain, 4000, valid_sets = [dtrain, dvalid])\n",
        "\n",
        "feature_importances = lgb_clf.feature_importance()\n",
        "\n",
        "y_pred_valid = lgb_clf.predict(X_test)\n",
        "score = roc_auc_score(y_true, y_pred_valid)\n",
        "\n",
        "print(f\"AUC = {score}\")"
      ],
      "metadata": {
        "id": "T5oFMrf8nGk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #wonchoi - Hyperparameter tuning for RF\n",
        "\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Prior to hyperparameter tuning \n",
        "# rf = RandomForestRegressor(n_estimators= 100, max_depth = 2, oob_score = True, random_state = 30000)\n",
        "\n",
        "# # Create a parameter grid to conduct a grid search for hyperparameters\n",
        "# param_grid = { 'bootstrap' : [True], \n",
        "#            'max_depth': [3, 5, 6, 10, 15, 20],\n",
        "#            'n_estimators': [100, 500, 1000]}\n",
        "\n",
        "# # Instantiate the grid search model\n",
        "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 0)\n",
        "\n",
        "# # Fit the grid search to the data and calculate its oob score\n",
        "# grid_search.fit(X_train, y_train)\n",
        "# grid_search.best_params_"
      ],
      "metadata": {
        "id": "NnT7DICEAUoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wonchoi - RF Model\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Create a random forest regressor model with the best hyperparameter fits \n",
        "rf_final = RandomForestRegressor(n_estimators = 100, max_depth = 5, n_jobs = -1, oob_score = True, random_state = 30000)\n",
        "rf_final = rf_final.fit(X_train, y_train)\n",
        "y_pred = rf_final.predict(X_train)\n",
        "score = roc_auc_score(y_train, y_pred)\n",
        "\n",
        "# Print AUC score\n",
        "print(f\"AUC = {score}\")\n",
        "\n",
        "# Calculate rmse\n",
        "res_rmse = mse(y_train, y_pred, squared = True)\n",
        "\n",
        "# Calculate oob score\n",
        "res_oob = rf_final.oob_score_\n",
        "print('Root mean squared error:', res_rmse,'Out of box score of final model: ', res_oob)"
      ],
      "metadata": {
        "id": "EJPsYforAYsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import required modules\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "#Build the model\n",
        "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "adaModel = ada_clf.fit(X_train, y_train)\n",
        "\n",
        "#Adaboost Score\n",
        "adaScore = adaModel.score(X_test, y_test)\n",
        "print(adaScore)"
      ],
      "metadata": {
        "id": "503CnwkFAdr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVMf-em_TXKe"
      },
      "source": [
        "### Model Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhrGl2hJevUP"
      },
      "outputs": [],
      "source": [
        "stack_models(('Nishi', xgb_cl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xai4ycpPe5hE"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/ID5059GroupProject')\n",
        "from utils.diagnositics import get_diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_diagnostics(('xgb', xgb_cl), y_true=y_test, X=X_test)"
      ],
      "metadata": {
        "id": "unbEcKHOcsR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YaEfw_VidjoB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}