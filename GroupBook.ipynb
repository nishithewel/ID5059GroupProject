{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### ID5059 KDDM\n",
    "### Assignment: P02\n",
    "### Group Name : Demented But Determined\n",
    "\n",
    "Group Members:\n",
    "- 200005828\n",
    "- 190001708\n",
    "- 210012057\n",
    "- 210003557\n",
    "- 210010283\n",
    "- 210005313\n",
    "\n",
    "<hr>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Summary\n",
    "\n",
    "The dataset consists of five files :\n",
    "1.\ttest_identity.csv\n",
    "    - Identity Dataset to test performance of model\n",
    "2.\ttest_transaction.csv\n",
    "    - Transaction Dataset to test performance of model\n",
    "3.\ttrain_identity.csv\n",
    "    - Identity Dataset to be used for model training\n",
    "4.\ttrain_transaction.csv\n",
    "    - Transaction Dataset to be used for model training\n",
    "5.\tsample_submission.csv\n",
    "    - File containing prediction results\n",
    "\n",
    "\n",
    "Column names in train and test identity dataset has different formats.\n",
    "We renamed test identity columns name to match to that of train column names.\n",
    "\n",
    "## Transaction Dataset :\n",
    "\n",
    "Columns in the transaction dataset can be grouped as below:\n",
    "\n",
    "1.\tTransactionID\n",
    "    -\tID of the transaction and is used to connect to identity dataset\n",
    "2.\tisFraud\n",
    "    -\tIndicating if the transaction is fraud or not\n",
    "3.\tTransactionAmt\n",
    "    -\tTransaction amount (in USD)\n",
    "4.\tProductCD\n",
    "    - Product code  with 5 distinct categories\n",
    "    - Categorical Features\n",
    "5.\tcard1 -  card6\n",
    "    - Payment card information\n",
    "    - Card 4 contains 4 distinct categories of the card used for the transaction\n",
    "    - Card5 has 120 unique values\n",
    "    - Card6 has 4 distinct categories\n",
    "    - Categorical Features\n",
    "    - Actual values are masked for some columns\n",
    "6.\taddr1 - addr2\n",
    "    - addr1 - billing region\n",
    "    - addr2 -  billing country\n",
    "    - Categorical Features\n",
    "7.\tdist1 - dist2\n",
    "    - Distance\n",
    "    - Categorical Features\n",
    "8.\tP_emaildomain\n",
    "    - purchaser email domains\n",
    "    - Categorical Features\n",
    "9.\tR_emaildomain\n",
    "    - recipient email domains\n",
    "    - Categorical Features\n",
    "10.\tC1 - C14\n",
    "    - counting, such as how many addresses are found to be associated with the payment card, etc.\n",
    "    - The actual meaning is masked\n",
    "11.\tD1 - D15\n",
    "    - timedelta, such as days between previous transaction, etc\n",
    "12.\tM1 - M9\n",
    "    - match, such as names on card and address, etc.\n",
    "    - Categorical Features\n",
    "13.\tV1 - V339\n",
    "    - Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "\n",
    "\n",
    "## Identity Dataset :\n",
    "\n",
    "1.\tTransactionID\n",
    "    - ID of the transaction and is used to connect to transaction dataset\n",
    "2.\tDeviceType\n",
    "    - Device type used for transaction mobile or desktop\n",
    "    - Categorical Features\n",
    "3.\tDeviceInfo\n",
    "    - Information about the device used for transaction\n",
    "    -   Categorical Features\n",
    "4.\tid-01 to id-38\n",
    "    - Masked information about the device\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn import over_sampling\n",
    "from utils.preprocess import preprocess\n",
    "from utils.reduce_memory import reduce_mem_usage\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the data\n",
    "\n",
    "Download the dataset from Kaggle and unzip it 'data' folder parallel to the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "root_dir = './data/'\n",
    "test_iden = pd.read_csv(root_dir + 'test_identity.csv')\n",
    "test_tran = pd.read_csv(root_dir + 'test_transaction.csv')\n",
    "train_iden = pd.read_csv(root_dir + 'train_identity.csv')\n",
    "train_tran = pd.read_csv(root_dir + 'train_transaction.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the large size, to keep loading size down we optimize data types to reduce memory usage."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Type down-casting  : The shape of the Train (590540, 394) , Test (506691, 393)\n",
      "Memory usage of dataframe is 1519.24 MB\n",
      "Memory usage after optimization is: 425.24 MB\n",
      "Decreased by 72.0%\n",
      "Memory usage of dataframe is 1775.15 MB\n",
      "Memory usage after optimization is: 487.16 MB\n",
      "Decreased by 72.6%\n",
      "Before Type down-casting  : The shape of the Train (590540, 394) , Test (506691, 393)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before Type down-casting  : The shape of the Train {train_tran.shape} , Test {test_tran.shape}\")\n",
    "test_tran = reduce_mem_usage(test_tran)\n",
    "train_tran = reduce_mem_usage(train_tran)\n",
    "print(f\"Before Type down-casting  : The shape of the Train {train_tran.shape} , Test {test_tran.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Merge the Transaction and Identity data set to create the 'Test' set.\n",
    "train = pd.merge(train_tran, train_iden, on = 'TransactionID', how = 'left')\n",
    "test = pd.merge(test_tran, test_iden, on = 'TransactionID', how = 'left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging  : The shape of the Train (590540, 434) , Test (506691, 433)\n"
     ]
    }
   ],
   "source": [
    "print(f\"After merging  : The shape of the Train {train.shape} , Test {test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size(472432, 434)\n",
      "DevTest size(118108, 434)\n"
     ]
    }
   ],
   "source": [
    "SEED= 42\n",
    "train, dev_test = train_test_split(train, test_size=0.2, random_state=int(SEED))\n",
    "\n",
    "print(\"Train size\"+ str(train.shape))\n",
    "print(\"DevTest size\"+ str(dev_test.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "del train_tran, train_iden, test_iden, test_tran"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test_dataset and train_dataset had a mismatch in the name of id features.\n",
    "In the train_dataset id features were present with the name id_x where x was a value between\n",
    "01 and 38 whereas in the test_dataset id features were of the form id-x.\n",
    "So, we changed the format of id features in the test_dataset from id-x to id_x."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "test.columns  = [col.replace('-','_') for col in test.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "           D1     D2    D3     D4     D5     D6     D7      D8    D9    D10  \\\n5307      0.0    NaN   NaN    0.0    NaN    NaN    NaN     NaN   NaN    0.0   \n191582    0.0    NaN   NaN    0.0    NaN    0.0    NaN     NaN   NaN    0.0   \n260168    7.0    7.0   6.0  420.0    6.0    NaN    NaN     NaN   NaN   71.0   \n18516   211.0  211.0   NaN  211.0  211.0  211.0  211.0  162.75  0.75  211.0   \n47538     0.0    NaN   NaN    NaN    NaN    NaN    NaN  204.00  0.00    NaN   \n62390     0.0    NaN   NaN    0.0    NaN    NaN    NaN     NaN   NaN    0.0   \n288883    0.0    NaN   NaN    0.0    NaN    NaN    NaN     NaN   NaN    0.0   \n397255    0.0    NaN   NaN    NaN    NaN    NaN    NaN     NaN   NaN    0.0   \n465998   13.0   13.0  14.0   14.0   14.0    NaN    NaN     NaN   NaN   14.0   \n323478    0.0    NaN   NaN    0.0    NaN    NaN    NaN     NaN   NaN    0.0   \n\n         D11  D12  D13  D14  \n5307     0.0  NaN  NaN  NaN  \n191582   NaN  0.0  0.0  0.0  \n260168   NaN  NaN  NaN  NaN  \n18516    NaN  NaN  0.0  NaN  \n47538    NaN  NaN  NaN  NaN  \n62390    0.0  NaN  NaN  NaN  \n288883   NaN  NaN  NaN  NaN  \n397255   0.0  NaN  NaN  NaN  \n465998  14.0  NaN  NaN  NaN  \n323478   0.0  NaN  NaN  NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>D1</th>\n      <th>D2</th>\n      <th>D3</th>\n      <th>D4</th>\n      <th>D5</th>\n      <th>D6</th>\n      <th>D7</th>\n      <th>D8</th>\n      <th>D9</th>\n      <th>D10</th>\n      <th>D11</th>\n      <th>D12</th>\n      <th>D13</th>\n      <th>D14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5307</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>191582</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>260168</th>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>420.0</td>\n      <td>6.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>71.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18516</th>\n      <td>211.0</td>\n      <td>211.0</td>\n      <td>NaN</td>\n      <td>211.0</td>\n      <td>211.0</td>\n      <td>211.0</td>\n      <td>211.0</td>\n      <td>162.75</td>\n      <td>0.75</td>\n      <td>211.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>47538</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>204.00</td>\n      <td>0.00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>62390</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>288883</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>397255</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>465998</th>\n      <td>13.0</td>\n      <td>13.0</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.0</td>\n      <td>14.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>323478</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, the problem here is that D features are mostly NaNs!\n",
    "d_features=[\"D\"+str(x) for x in range(1,15)]\n",
    "train[d_features].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "         M1   M2   M3   M4   M5   M6   M7   M8\n5307      T    T    T   M0    T    T    F    F\n191582  NaN  NaN  NaN   M2  NaN  NaN  NaN  NaN\n260168  NaN  NaN  NaN  NaN  NaN    F  NaN  NaN\n18516   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n47538   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n62390     T    T    T  NaN  NaN  NaN  NaN  NaN\n288883  NaN  NaN  NaN  NaN  NaN    T  NaN  NaN\n397255    T    T    T  NaN  NaN    F    F    F\n465998    T    T    T  NaN  NaN    F    F    F\n323478    T    T    T   M0    F    F  NaN  NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>M1</th>\n      <th>M2</th>\n      <th>M3</th>\n      <th>M4</th>\n      <th>M5</th>\n      <th>M6</th>\n      <th>M7</th>\n      <th>M8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5307</th>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>M0</td>\n      <td>T</td>\n      <td>T</td>\n      <td>F</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>191582</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>260168</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18516</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>47538</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>62390</th>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>288883</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>T</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>397255</th>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>F</td>\n      <td>F</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>465998</th>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>F</td>\n      <td>F</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>323478</th>\n      <td>T</td>\n      <td>T</td>\n      <td>T</td>\n      <td>M0</td>\n      <td>F</td>\n      <td>F</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_features=[\"M\"+str(x) for x in range(1,9)]\n",
    "train[m_features].head(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['isFraud'].value_counts(normalize = True).plot(kind= 'barh')\n",
    "plt.title('% of Target class in training dataset')\n",
    "plt.ylabel('isFraud (1 = Fraud)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Preprocessing\n",
    "\n",
    "Systematically, deal with features by considering the following,\n",
    "- 'M' columns - look like true or false lets convert them to 1, 0\n",
    "- 'D' columns - these are fine all numerical\n",
    "- 'C' columns - numerical except for 4,6\n",
    "\n",
    "We deal with categorical variables using dummy vars."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "dev_test = preprocess(dev_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "        TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  \\\n5307          2992307        0         174911         59.0000         W   \n191582        3178582        0        4301977         23.3750         C   \n260168        3247168        0        6229929         35.9375         W   \n18516         3005516        0         497176        100.0000         R   \n47538         3034538        0        1124702         75.0000         H   \n\n        card1  card2  card3       card4  card5  ... id_02_to_mean_card4  \\\n5307     4988  334.0  150.0        visa  226.0  ...                 NaN   \n191582   3867  296.0  185.0        visa  226.0  ...            2.700445   \n260168  12577  268.0  150.0        visa  166.0  ...                 NaN   \n18516    6019  583.0  150.0        visa  226.0  ...            0.579748   \n47538   16075  514.0  150.0  mastercard  102.0  ...            0.481128   \n\n       id_02_to_std_card4 id_02_to_mean_addr1  id_02_to_std_addr1  \\\n5307                  NaN                 NaN                 NaN   \n191582           2.944736                 NaN                 NaN   \n260168                NaN                 NaN                 NaN   \n18516            0.632194            0.815571            0.858382   \n47538            0.548746            0.815978            0.835990   \n\n        D15_to_mean_card1 D15_to_std_card1 D15_to_mean_card4  \\\n5307             0.000000         0.000000          0.000000   \n191582           0.000000         0.000000          0.000000   \n260168           2.597656         2.148405          2.490234   \n18516            0.000000         0.000000          0.000000   \n47538                 NaN              NaN               NaN   \n\n        D15_to_std_card4  D15_to_mean_addr1  D15_to_std_addr1  \n5307            0.000000           0.000000          0.000000  \n191582          0.000000                NaN               NaN  \n260168          2.056071           2.582031          2.174831  \n18516           0.000000           0.000000          0.000000  \n47538                NaN                NaN               NaN  \n\n[5 rows x 471 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionID</th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card4</th>\n      <th>card5</th>\n      <th>...</th>\n      <th>id_02_to_mean_card4</th>\n      <th>id_02_to_std_card4</th>\n      <th>id_02_to_mean_addr1</th>\n      <th>id_02_to_std_addr1</th>\n      <th>D15_to_mean_card1</th>\n      <th>D15_to_std_card1</th>\n      <th>D15_to_mean_card4</th>\n      <th>D15_to_std_card4</th>\n      <th>D15_to_mean_addr1</th>\n      <th>D15_to_std_addr1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5307</th>\n      <td>2992307</td>\n      <td>0</td>\n      <td>174911</td>\n      <td>59.0000</td>\n      <td>W</td>\n      <td>4988</td>\n      <td>334.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>226.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>191582</th>\n      <td>3178582</td>\n      <td>0</td>\n      <td>4301977</td>\n      <td>23.3750</td>\n      <td>C</td>\n      <td>3867</td>\n      <td>296.0</td>\n      <td>185.0</td>\n      <td>visa</td>\n      <td>226.0</td>\n      <td>...</td>\n      <td>2.700445</td>\n      <td>2.944736</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>260168</th>\n      <td>3247168</td>\n      <td>0</td>\n      <td>6229929</td>\n      <td>35.9375</td>\n      <td>W</td>\n      <td>12577</td>\n      <td>268.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>166.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.597656</td>\n      <td>2.148405</td>\n      <td>2.490234</td>\n      <td>2.056071</td>\n      <td>2.582031</td>\n      <td>2.174831</td>\n    </tr>\n    <tr>\n      <th>18516</th>\n      <td>3005516</td>\n      <td>0</td>\n      <td>497176</td>\n      <td>100.0000</td>\n      <td>R</td>\n      <td>6019</td>\n      <td>583.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>226.0</td>\n      <td>...</td>\n      <td>0.579748</td>\n      <td>0.632194</td>\n      <td>0.815571</td>\n      <td>0.858382</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>47538</th>\n      <td>3034538</td>\n      <td>0</td>\n      <td>1124702</td>\n      <td>75.0000</td>\n      <td>H</td>\n      <td>16075</td>\n      <td>514.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>102.0</td>\n      <td>...</td>\n      <td>0.481128</td>\n      <td>0.548746</td>\n      <td>0.815978</td>\n      <td>0.835990</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 471 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What features are we dropping and why?\n",
    "\n",
    "TransactionID - Only a unique identifier for identity.\n",
    "\n",
    "Id Columns - All Id columns contain at least 75% NaN values\n",
    "\n",
    "V Columns - V columns consists of Vesta engineered features. They are large is number, have additional training overhead\n",
    "            and do not add any significant performance improvement (As per Kaggle data discussion )."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:title={'center':'NaN as a % in ID columns'}>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_cols = [col for col in train.columns if col.startswith('id')]\n",
    "df_id = train[id_cols]\n",
    "nulls = df_id.isnull().sum()/len(df_id) #as a percentage\n",
    "nulls.sort_values(ascending=False).plot.bar(title='NaN as a % in ID columns')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "v_cols = [col for col in train.columns if col.startswith('V')]\n",
    "train.drop(v_cols,axis =1, inplace = True)\n",
    "train.drop(id_cols, axis = 1,inplace = True)\n",
    "test.drop(v_cols,axis =1, inplace = True)\n",
    "test.drop(id_cols, axis = 1,inplace = True)\n",
    "dev_test.drop(v_cols,axis =1, inplace = True)\n",
    "dev_test.drop(id_cols, axis = 1,inplace = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "categorical_features = list(set(train.columns) - set(train._get_numeric_data().columns))\n",
    "#we get dummy for some features only\n",
    "all_cat_col_names = [ col for col in categorical_features\n",
    "                    if len(train[col].unique()) < 15]\n",
    "\n",
    "large_cats_col_names = list(set(train.columns) - set(train._get_numeric_data()) - set(all_cat_col_names))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One hot encoding of all the categorical variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding OS_id_30\n",
      "encoding M2\n",
      "encoding M7\n",
      "encoding R_emaildomain_3\n",
      "encoding card6\n",
      "encoding P_emaildomain_2\n",
      "encoding M4\n",
      "encoding card4\n",
      "encoding DeviceType\n",
      "encoding ProductCD\n",
      "encoding M8\n",
      "encoding M6\n",
      "encoding M9\n",
      "encoding M3\n",
      "encoding R_emaildomain_2\n",
      "encoding M5\n",
      "encoding P_emaildomain_3\n",
      "encoding device_name\n",
      "encoding M1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder  #the disco import\n",
    "def dummy_transform(df,encoder,col):\n",
    "  new_col = [col+'_' + str(i)  for i in encoder.categories_[0].tolist()]\n",
    "  dummy_arr = encoder.transform(df[col].to_numpy().reshape(-1, 1))\n",
    "  dummy_df = pd.DataFrame(dummy_arr, columns = new_col)\n",
    "\n",
    "  # Allows nice concat-ing\n",
    "  dummy_df.reset_index(drop=True, inplace=True)\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "  return pd.concat([df, dummy_df],axis =1)\n",
    "\n",
    "\n",
    "for col in all_cat_col_names:\n",
    "  print('encoding',col)\n",
    "  enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "  dummy_train = enc.fit_transform(train[col].to_numpy().reshape(-1, 1))\n",
    "\n",
    "  train = dummy_transform(train,enc,col)\n",
    "  dev_test = dummy_transform(dev_test,enc,col)\n",
    "  test = dummy_transform(test,enc,col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "((472432, 187), (118108, 187), (506691, 186))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train.shape, dev_test.shape,test.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: The shape of the Train (472432, 187) \n",
      "After dropping: The shape of the Train (472432, 168) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Before dropping: The shape of the Train {train.shape} \")\n",
    "\n",
    "train.drop(all_cat_col_names,axis =1,inplace = True )\n",
    "print(f\"After dropping: The shape of the Train {train.shape} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: The shape of the test (506691, 186) \n",
      "After dropping: The shape of the test (506691, 167) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Before dropping: The shape of the test {test.shape} \")\n",
    "\n",
    "test.drop(all_cat_col_names,axis =1,inplace = True )\n",
    "print(f\"After dropping: The shape of the test {test.shape} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping: The shape of the test (118108, 187) \n",
      "After dropping: The shape of the test (118108, 168) \n"
     ]
    }
   ],
   "source": [
    "print(f\"Before dropping: The shape of the test {dev_test.shape} \")\n",
    "dev_test.drop(all_cat_col_names,axis =1,inplace = True )\n",
    "print(f\"After dropping: The shape of the test {dev_test.shape} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "   card1  card2  card3  card5  card6_charge card  card6_credit  card6_debit  \\\n0   4988  334.0  150.0  226.0                0.0           0.0          1.0   \n1   3867  296.0  185.0  226.0                0.0           1.0          0.0   \n2  12577  268.0  150.0  166.0                0.0           0.0          1.0   \n3   6019  583.0  150.0  226.0                0.0           1.0          0.0   \n4  16075  514.0  150.0  102.0                0.0           1.0          0.0   \n\n   card6_debit or credit  card6_nan  card4_american express  card4_discover  \\\n0                    0.0        0.0                     0.0             0.0   \n1                    0.0        0.0                     0.0             0.0   \n2                    0.0        0.0                     0.0             0.0   \n3                    0.0        0.0                     0.0             0.0   \n4                    0.0        0.0                     0.0             0.0   \n\n   card4_mastercard  card4_visa  card4_nan  \n0               0.0         1.0        0.0  \n1               0.0         1.0        0.0  \n2               0.0         1.0        0.0  \n3               0.0         1.0        0.0  \n4               1.0         0.0        0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card5</th>\n      <th>card6_charge card</th>\n      <th>card6_credit</th>\n      <th>card6_debit</th>\n      <th>card6_debit or credit</th>\n      <th>card6_nan</th>\n      <th>card4_american express</th>\n      <th>card4_discover</th>\n      <th>card4_mastercard</th>\n      <th>card4_visa</th>\n      <th>card4_nan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4988</td>\n      <td>334.0</td>\n      <td>150.0</td>\n      <td>226.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3867</td>\n      <td>296.0</td>\n      <td>185.0</td>\n      <td>226.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12577</td>\n      <td>268.0</td>\n      <td>150.0</td>\n      <td>166.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6019</td>\n      <td>583.0</td>\n      <td>150.0</td>\n      <td>226.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16075</td>\n      <td>514.0</td>\n      <td>150.0</td>\n      <td>102.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[c for c in train.columns if c.startswith('card')]\n",
    "train[a].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1], dtype=int8)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['isFraud'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "((472432, 168), (118108, 168))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train.shape, dev_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to deal with the variables with high number of cats."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def frequency_encoder(x,groupby):\n",
    "  if x == None:\n",
    "    return groupby.get(np.nan)\n",
    "  else:\n",
    "    return groupby.get(x)\n",
    "\n",
    "\n",
    "  # large_cats\n",
    "for col in large_cats_col_names:\n",
    "  freq_enc = (train.groupby(col,dropna=False  # to prevent loss of information\n",
    "                            ).size()) / len(train)\n",
    "  train[col+'freq'] = train[col].apply(lambda x : frequency_encoder(x,freq_enc))\n",
    "  test[col+'freq'] = test[col].apply(lambda x : frequency_encoder(x,freq_enc))\n",
    "  dev_test[col+'freq'] = dev_test[col].apply(lambda x : frequency_encoder(x,freq_enc))\n",
    "\n",
    "train.drop(large_cats_col_names,axis = 1,inplace = True)\n",
    "test.drop(large_cats_col_names,axis = 1,inplace = True)\n",
    "dev_test.drop(large_cats_col_names,axis = 1,inplace = True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Column Imputation\n",
    "\n",
    "How are we dealing with NaN/Missing/Infinite Values?\n",
    "\n",
    "\n",
    "To use SMOTE we must remove np.infs and NaNs,\n",
    "  - So we initially drop columns with less than 80% non-NA values.\n",
    "  - Then, we drop rows containing NaN values\n",
    "\n",
    "Using this resulted in a higher performance on balanced score measures than\n",
    "keeping NaNs and not using SMOTE.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "high_NA_columns = train.columns[train.isnull().sum()/len(train)>0.2]\n",
    "## We need train_na asa separate DF for further processing.\n",
    "train_na = train.drop(high_NA_columns, axis=1)\n",
    "test= test.drop(high_NA_columns, axis=1)\n",
    "dev_test = dev_test.drop(high_NA_columns, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nulls = train_na.isnull().sum()/len(train_na) #as a percentage\n",
    "# nulls.sort_values(ascending=False)\n",
    "# # nulls.sort_values(ascending=False).plot.barh(title='NaN as a %')\n",
    "\n",
    "nulls = train_na.isnull().sum()/len(train_na) #as a percentage\n",
    "nulls[nulls.values > 0].sort_values(ascending=False).plot.bar()\n",
    "plt.title('Columns with non-zero NaN %')\n",
    "plt.ylabel('NaNs as %')\n",
    "plt.show()\n",
    "\n",
    "train_no_na = train_na[~train_na.isin([np.nan, np.inf, -np.inf]).any(1)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outlier removal\n",
    "\n",
    "Since we are using ensemble tree methods that are robust to\n",
    "outliers we choose not to perform outlier removal. Doing it in a\n",
    "feature agnostic manner would destroy information on anomalies and doing\n",
    "so otherwise would require pruning each column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature engineering (Creation)\n",
    "##### Splitting email domains\n",
    "       - Domain name of an email looks like important feature and we extracted the domain\n",
    "         from\n",
    "##### Checking decimals in the transaction Amount\n",
    "##### Extracting device info\n",
    "      - device name\n",
    "          Extaction\n",
    "          Correction\n",
    "      - device version\n",
    "      - Operating system ID\n",
    "      - Operating system version\n",
    "      - browser ID\n",
    "      - browser version\n",
    "      - screen width\n",
    "      - screen height\n",
    "\n",
    "#### Aggregating columns to generate new features\n",
    "      Generating new features by dividing certain numerical features by calculating\n",
    "      certain groups' mean or std value of different features\n",
    "#### Changing data types of categorical columns to category\n",
    "      Generating new features to make sure that every cell has only one value,\n",
    "      such as splitting “id_30” into “screen_width” and “screen_height”\n",
    "#### Converting M columns to 1 and 0\n",
    "      Replacing the values of M columns- object features to 0 and 1\n",
    "#### Transaction Amount Transformations\n",
    "      Generating new features to get the number of decimal places in the transaction amount,\n",
    "      the decimal value of the transaction amount and the logarithm of transaction amount"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class Balancing\n",
    "To perform class balancing we use SMOTE from Imbalanced Learn, to create samples\n",
    "of minority data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Class balancing only for train data for comparing it later on.\n",
    "\n",
    "train_no_na = train_na[~train_na.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "test = test[~test.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "dev_test = dev_test[~dev_test.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "# train_no_na = train_no_na._get_numeric_data()\n",
    "X_train, y_train = train_no_na.drop('isFraud',axis = 1), train_no_na['isFraud']\n",
    "oversample = over_sampling.SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "X_train.drop('TransactionID', axis=1, inplace = True)\n",
    "\n",
    "# Dev test data for model checking\n",
    "X_test, y_test = dev_test.drop(['isFraud', 'TransactionID'], axis = 1), dev_test['isFraud']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize = True).plot(kind= 'barh')\n",
    "plt.title('% of class')\n",
    "plt.ylabel('isFraud (1 = Fraud)')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Creation & Fitting\n",
    "### Individual models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost model fitting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112]\tvalidation_0-auc:0.98310\tvalidation_1-auc:0.80764\n",
      "[113]\tvalidation_0-auc:0.98319\tvalidation_1-auc:0.80794\n",
      "[114]\tvalidation_0-auc:0.98326\tvalidation_1-auc:0.80820\n",
      "[115]\tvalidation_0-auc:0.98332\tvalidation_1-auc:0.80828\n",
      "[116]\tvalidation_0-auc:0.98342\tvalidation_1-auc:0.80854\n",
      "[117]\tvalidation_0-auc:0.98348\tvalidation_1-auc:0.80882\n",
      "[118]\tvalidation_0-auc:0.98353\tvalidation_1-auc:0.80891\n",
      "[119]\tvalidation_0-auc:0.98360\tvalidation_1-auc:0.80905\n",
      "[120]\tvalidation_0-auc:0.98365\tvalidation_1-auc:0.80912\n",
      "[121]\tvalidation_0-auc:0.98372\tvalidation_1-auc:0.80943\n",
      "[122]\tvalidation_0-auc:0.98376\tvalidation_1-auc:0.80951\n",
      "[123]\tvalidation_0-auc:0.98386\tvalidation_1-auc:0.80976\n",
      "[124]\tvalidation_0-auc:0.98391\tvalidation_1-auc:0.80983\n",
      "[125]\tvalidation_0-auc:0.98400\tvalidation_1-auc:0.81012\n",
      "[126]\tvalidation_0-auc:0.98403\tvalidation_1-auc:0.81020\n",
      "[127]\tvalidation_0-auc:0.98411\tvalidation_1-auc:0.81046\n",
      "[128]\tvalidation_0-auc:0.98415\tvalidation_1-auc:0.81054\n",
      "[129]\tvalidation_0-auc:0.98421\tvalidation_1-auc:0.81080\n",
      "[130]\tvalidation_0-auc:0.98426\tvalidation_1-auc:0.81093\n",
      "[131]\tvalidation_0-auc:0.98436\tvalidation_1-auc:0.81125\n",
      "[132]\tvalidation_0-auc:0.98438\tvalidation_1-auc:0.81126\n",
      "[133]\tvalidation_0-auc:0.98447\tvalidation_1-auc:0.81145\n",
      "[134]\tvalidation_0-auc:0.98452\tvalidation_1-auc:0.81156\n",
      "[135]\tvalidation_0-auc:0.98460\tvalidation_1-auc:0.81186\n",
      "[136]\tvalidation_0-auc:0.98463\tvalidation_1-auc:0.81191\n",
      "[137]\tvalidation_0-auc:0.98467\tvalidation_1-auc:0.81210\n",
      "[138]\tvalidation_0-auc:0.98471\tvalidation_1-auc:0.81219\n",
      "[139]\tvalidation_0-auc:0.98481\tvalidation_1-auc:0.81244\n",
      "[140]\tvalidation_0-auc:0.98483\tvalidation_1-auc:0.81251\n",
      "[141]\tvalidation_0-auc:0.98486\tvalidation_1-auc:0.81256\n",
      "[142]\tvalidation_0-auc:0.98492\tvalidation_1-auc:0.81276\n",
      "[143]\tvalidation_0-auc:0.98496\tvalidation_1-auc:0.81289\n",
      "[144]\tvalidation_0-auc:0.98504\tvalidation_1-auc:0.81302\n",
      "[145]\tvalidation_0-auc:0.98510\tvalidation_1-auc:0.81329\n",
      "[146]\tvalidation_0-auc:0.98517\tvalidation_1-auc:0.81352\n",
      "[147]\tvalidation_0-auc:0.98525\tvalidation_1-auc:0.81387\n",
      "[148]\tvalidation_0-auc:0.98532\tvalidation_1-auc:0.81402\n",
      "[149]\tvalidation_0-auc:0.98534\tvalidation_1-auc:0.81406\n",
      "[150]\tvalidation_0-auc:0.98542\tvalidation_1-auc:0.81421\n",
      "[151]\tvalidation_0-auc:0.98546\tvalidation_1-auc:0.81425\n",
      "[152]\tvalidation_0-auc:0.98552\tvalidation_1-auc:0.81440\n",
      "[153]\tvalidation_0-auc:0.98556\tvalidation_1-auc:0.81468\n",
      "[154]\tvalidation_0-auc:0.98564\tvalidation_1-auc:0.81489\n",
      "[155]\tvalidation_0-auc:0.98566\tvalidation_1-auc:0.81499\n",
      "[156]\tvalidation_0-auc:0.98569\tvalidation_1-auc:0.81509\n",
      "[157]\tvalidation_0-auc:0.98576\tvalidation_1-auc:0.81524\n",
      "[158]\tvalidation_0-auc:0.98577\tvalidation_1-auc:0.81523\n",
      "[159]\tvalidation_0-auc:0.98583\tvalidation_1-auc:0.81528\n",
      "[160]\tvalidation_0-auc:0.98595\tvalidation_1-auc:0.81559\n",
      "[161]\tvalidation_0-auc:0.98598\tvalidation_1-auc:0.81564\n",
      "[162]\tvalidation_0-auc:0.98603\tvalidation_1-auc:0.81589\n",
      "[163]\tvalidation_0-auc:0.98608\tvalidation_1-auc:0.81614\n",
      "[164]\tvalidation_0-auc:0.98612\tvalidation_1-auc:0.81617\n",
      "[165]\tvalidation_0-auc:0.98618\tvalidation_1-auc:0.81634\n",
      "[166]\tvalidation_0-auc:0.98620\tvalidation_1-auc:0.81638\n",
      "[167]\tvalidation_0-auc:0.98627\tvalidation_1-auc:0.81670\n",
      "[168]\tvalidation_0-auc:0.98632\tvalidation_1-auc:0.81668\n",
      "[169]\tvalidation_0-auc:0.98637\tvalidation_1-auc:0.81691\n",
      "[170]\tvalidation_0-auc:0.98638\tvalidation_1-auc:0.81684\n",
      "[171]\tvalidation_0-auc:0.98645\tvalidation_1-auc:0.81712\n",
      "[172]\tvalidation_0-auc:0.98646\tvalidation_1-auc:0.81706\n",
      "[173]\tvalidation_0-auc:0.98652\tvalidation_1-auc:0.81731\n",
      "[174]\tvalidation_0-auc:0.98657\tvalidation_1-auc:0.81734\n",
      "[175]\tvalidation_0-auc:0.98661\tvalidation_1-auc:0.81749\n",
      "[176]\tvalidation_0-auc:0.98670\tvalidation_1-auc:0.81766\n",
      "[177]\tvalidation_0-auc:0.98677\tvalidation_1-auc:0.81793\n",
      "[178]\tvalidation_0-auc:0.98683\tvalidation_1-auc:0.81811\n",
      "[179]\tvalidation_0-auc:0.98687\tvalidation_1-auc:0.81813\n",
      "[180]\tvalidation_0-auc:0.98694\tvalidation_1-auc:0.81833\n",
      "[181]\tvalidation_0-auc:0.98697\tvalidation_1-auc:0.81838\n",
      "[182]\tvalidation_0-auc:0.98700\tvalidation_1-auc:0.81853\n",
      "[183]\tvalidation_0-auc:0.98707\tvalidation_1-auc:0.81871\n",
      "[184]\tvalidation_0-auc:0.98715\tvalidation_1-auc:0.81885\n",
      "[185]\tvalidation_0-auc:0.98718\tvalidation_1-auc:0.81897\n",
      "[186]\tvalidation_0-auc:0.98724\tvalidation_1-auc:0.81900\n",
      "[187]\tvalidation_0-auc:0.98730\tvalidation_1-auc:0.81916\n",
      "[188]\tvalidation_0-auc:0.98735\tvalidation_1-auc:0.81927\n",
      "[189]\tvalidation_0-auc:0.98742\tvalidation_1-auc:0.81947\n",
      "[190]\tvalidation_0-auc:0.98747\tvalidation_1-auc:0.81961\n",
      "[191]\tvalidation_0-auc:0.98751\tvalidation_1-auc:0.81980\n",
      "[192]\tvalidation_0-auc:0.98755\tvalidation_1-auc:0.81995\n",
      "[193]\tvalidation_0-auc:0.98763\tvalidation_1-auc:0.82003\n",
      "[194]\tvalidation_0-auc:0.98766\tvalidation_1-auc:0.82004\n",
      "[195]\tvalidation_0-auc:0.98771\tvalidation_1-auc:0.82026\n",
      "[196]\tvalidation_0-auc:0.98776\tvalidation_1-auc:0.82041\n",
      "[197]\tvalidation_0-auc:0.98780\tvalidation_1-auc:0.82049\n",
      "[198]\tvalidation_0-auc:0.98784\tvalidation_1-auc:0.82069\n",
      "[199]\tvalidation_0-auc:0.98787\tvalidation_1-auc:0.82092\n",
      "[200]\tvalidation_0-auc:0.98793\tvalidation_1-auc:0.82098\n",
      "[201]\tvalidation_0-auc:0.98798\tvalidation_1-auc:0.82124\n",
      "[202]\tvalidation_0-auc:0.98801\tvalidation_1-auc:0.82140\n",
      "[203]\tvalidation_0-auc:0.98803\tvalidation_1-auc:0.82139\n",
      "[204]\tvalidation_0-auc:0.98804\tvalidation_1-auc:0.82152\n",
      "[205]\tvalidation_0-auc:0.98809\tvalidation_1-auc:0.82161\n",
      "[206]\tvalidation_0-auc:0.98815\tvalidation_1-auc:0.82183\n",
      "[207]\tvalidation_0-auc:0.98819\tvalidation_1-auc:0.82194\n",
      "[208]\tvalidation_0-auc:0.98821\tvalidation_1-auc:0.82201\n",
      "[209]\tvalidation_0-auc:0.98824\tvalidation_1-auc:0.82203\n",
      "[210]\tvalidation_0-auc:0.98828\tvalidation_1-auc:0.82226\n",
      "[211]\tvalidation_0-auc:0.98830\tvalidation_1-auc:0.82229\n",
      "[212]\tvalidation_0-auc:0.98836\tvalidation_1-auc:0.82243\n",
      "[213]\tvalidation_0-auc:0.98838\tvalidation_1-auc:0.82243\n",
      "[214]\tvalidation_0-auc:0.98842\tvalidation_1-auc:0.82261\n",
      "[215]\tvalidation_0-auc:0.98846\tvalidation_1-auc:0.82275\n",
      "[216]\tvalidation_0-auc:0.98850\tvalidation_1-auc:0.82276\n",
      "[217]\tvalidation_0-auc:0.98853\tvalidation_1-auc:0.82284\n",
      "[218]\tvalidation_0-auc:0.98856\tvalidation_1-auc:0.82296\n",
      "[219]\tvalidation_0-auc:0.98858\tvalidation_1-auc:0.82299\n",
      "[220]\tvalidation_0-auc:0.98861\tvalidation_1-auc:0.82316\n",
      "[221]\tvalidation_0-auc:0.98864\tvalidation_1-auc:0.82319\n",
      "[222]\tvalidation_0-auc:0.98868\tvalidation_1-auc:0.82330\n",
      "[223]\tvalidation_0-auc:0.98870\tvalidation_1-auc:0.82343\n",
      "[224]\tvalidation_0-auc:0.98876\tvalidation_1-auc:0.82345\n",
      "[225]\tvalidation_0-auc:0.98879\tvalidation_1-auc:0.82363\n",
      "[226]\tvalidation_0-auc:0.98882\tvalidation_1-auc:0.82376\n",
      "[227]\tvalidation_0-auc:0.98883\tvalidation_1-auc:0.82390\n",
      "[228]\tvalidation_0-auc:0.98889\tvalidation_1-auc:0.82396\n",
      "[229]\tvalidation_0-auc:0.98891\tvalidation_1-auc:0.82404\n",
      "[230]\tvalidation_0-auc:0.98894\tvalidation_1-auc:0.82408\n",
      "[231]\tvalidation_0-auc:0.98896\tvalidation_1-auc:0.82424\n",
      "[232]\tvalidation_0-auc:0.98901\tvalidation_1-auc:0.82431\n",
      "[233]\tvalidation_0-auc:0.98903\tvalidation_1-auc:0.82441\n",
      "[234]\tvalidation_0-auc:0.98907\tvalidation_1-auc:0.82448\n",
      "[235]\tvalidation_0-auc:0.98910\tvalidation_1-auc:0.82458\n",
      "[236]\tvalidation_0-auc:0.98912\tvalidation_1-auc:0.82460\n",
      "[237]\tvalidation_0-auc:0.98916\tvalidation_1-auc:0.82479\n",
      "[238]\tvalidation_0-auc:0.98922\tvalidation_1-auc:0.82486\n",
      "[239]\tvalidation_0-auc:0.98926\tvalidation_1-auc:0.82496\n",
      "[240]\tvalidation_0-auc:0.98929\tvalidation_1-auc:0.82507\n",
      "[241]\tvalidation_0-auc:0.98932\tvalidation_1-auc:0.82510\n",
      "[242]\tvalidation_0-auc:0.98935\tvalidation_1-auc:0.82521\n",
      "[243]\tvalidation_0-auc:0.98938\tvalidation_1-auc:0.82523\n",
      "[244]\tvalidation_0-auc:0.98940\tvalidation_1-auc:0.82535\n",
      "[245]\tvalidation_0-auc:0.98945\tvalidation_1-auc:0.82545\n",
      "[246]\tvalidation_0-auc:0.98950\tvalidation_1-auc:0.82556\n",
      "[247]\tvalidation_0-auc:0.98951\tvalidation_1-auc:0.82563\n",
      "[248]\tvalidation_0-auc:0.98960\tvalidation_1-auc:0.82581\n",
      "[249]\tvalidation_0-auc:0.98962\tvalidation_1-auc:0.82586\n",
      "[250]\tvalidation_0-auc:0.98964\tvalidation_1-auc:0.82591\n",
      "[251]\tvalidation_0-auc:0.98977\tvalidation_1-auc:0.82599\n",
      "[252]\tvalidation_0-auc:0.98981\tvalidation_1-auc:0.82609\n",
      "[253]\tvalidation_0-auc:0.98986\tvalidation_1-auc:0.82620\n",
      "[254]\tvalidation_0-auc:0.98987\tvalidation_1-auc:0.82624\n",
      "[255]\tvalidation_0-auc:0.98997\tvalidation_1-auc:0.82637\n",
      "[256]\tvalidation_0-auc:0.99005\tvalidation_1-auc:0.82657\n",
      "[257]\tvalidation_0-auc:0.99012\tvalidation_1-auc:0.82663\n",
      "[258]\tvalidation_0-auc:0.99016\tvalidation_1-auc:0.82682\n",
      "[259]\tvalidation_0-auc:0.99019\tvalidation_1-auc:0.82683\n",
      "[260]\tvalidation_0-auc:0.99021\tvalidation_1-auc:0.82691\n",
      "[261]\tvalidation_0-auc:0.99026\tvalidation_1-auc:0.82712\n",
      "[262]\tvalidation_0-auc:0.99028\tvalidation_1-auc:0.82720\n",
      "[263]\tvalidation_0-auc:0.99032\tvalidation_1-auc:0.82739\n",
      "[264]\tvalidation_0-auc:0.99042\tvalidation_1-auc:0.82757\n",
      "[265]\tvalidation_0-auc:0.99046\tvalidation_1-auc:0.82766\n",
      "[266]\tvalidation_0-auc:0.99050\tvalidation_1-auc:0.82783\n",
      "[267]\tvalidation_0-auc:0.99052\tvalidation_1-auc:0.82795\n",
      "[268]\tvalidation_0-auc:0.99059\tvalidation_1-auc:0.82810\n",
      "[269]\tvalidation_0-auc:0.99065\tvalidation_1-auc:0.82822\n",
      "[270]\tvalidation_0-auc:0.99071\tvalidation_1-auc:0.82847\n",
      "[271]\tvalidation_0-auc:0.99075\tvalidation_1-auc:0.82856\n",
      "[272]\tvalidation_0-auc:0.99079\tvalidation_1-auc:0.82861\n",
      "[273]\tvalidation_0-auc:0.99081\tvalidation_1-auc:0.82882\n",
      "[274]\tvalidation_0-auc:0.99085\tvalidation_1-auc:0.82888\n",
      "[275]\tvalidation_0-auc:0.99087\tvalidation_1-auc:0.82898\n",
      "[276]\tvalidation_0-auc:0.99091\tvalidation_1-auc:0.82910\n",
      "[277]\tvalidation_0-auc:0.99095\tvalidation_1-auc:0.82929\n",
      "[278]\tvalidation_0-auc:0.99100\tvalidation_1-auc:0.82932\n",
      "[279]\tvalidation_0-auc:0.99103\tvalidation_1-auc:0.82947\n",
      "[280]\tvalidation_0-auc:0.99107\tvalidation_1-auc:0.82955\n",
      "[281]\tvalidation_0-auc:0.99110\tvalidation_1-auc:0.82972\n",
      "[282]\tvalidation_0-auc:0.99116\tvalidation_1-auc:0.82971\n",
      "[283]\tvalidation_0-auc:0.99119\tvalidation_1-auc:0.82981\n",
      "[284]\tvalidation_0-auc:0.99122\tvalidation_1-auc:0.83002\n",
      "[285]\tvalidation_0-auc:0.99125\tvalidation_1-auc:0.83007\n",
      "[286]\tvalidation_0-auc:0.99128\tvalidation_1-auc:0.83024\n",
      "[287]\tvalidation_0-auc:0.99130\tvalidation_1-auc:0.83027\n",
      "[288]\tvalidation_0-auc:0.99135\tvalidation_1-auc:0.83036\n",
      "[289]\tvalidation_0-auc:0.99137\tvalidation_1-auc:0.83045\n",
      "[290]\tvalidation_0-auc:0.99143\tvalidation_1-auc:0.83065\n",
      "[291]\tvalidation_0-auc:0.99147\tvalidation_1-auc:0.83081\n",
      "[292]\tvalidation_0-auc:0.99149\tvalidation_1-auc:0.83090\n",
      "[293]\tvalidation_0-auc:0.99155\tvalidation_1-auc:0.83109\n",
      "[294]\tvalidation_0-auc:0.99160\tvalidation_1-auc:0.83116\n",
      "[295]\tvalidation_0-auc:0.99167\tvalidation_1-auc:0.83136\n",
      "[296]\tvalidation_0-auc:0.99171\tvalidation_1-auc:0.83149\n",
      "[297]\tvalidation_0-auc:0.99173\tvalidation_1-auc:0.83165\n",
      "[298]\tvalidation_0-auc:0.99176\tvalidation_1-auc:0.83182\n",
      "[299]\tvalidation_0-auc:0.99181\tvalidation_1-auc:0.83197\n",
      "[300]\tvalidation_0-auc:0.99184\tvalidation_1-auc:0.83218\n",
      "[301]\tvalidation_0-auc:0.99187\tvalidation_1-auc:0.83235\n",
      "[302]\tvalidation_0-auc:0.99191\tvalidation_1-auc:0.83247\n",
      "[303]\tvalidation_0-auc:0.99193\tvalidation_1-auc:0.83258\n",
      "[304]\tvalidation_0-auc:0.99196\tvalidation_1-auc:0.83273\n",
      "[305]\tvalidation_0-auc:0.99199\tvalidation_1-auc:0.83288\n",
      "[306]\tvalidation_0-auc:0.99203\tvalidation_1-auc:0.83300\n",
      "[307]\tvalidation_0-auc:0.99207\tvalidation_1-auc:0.83313\n",
      "[308]\tvalidation_0-auc:0.99209\tvalidation_1-auc:0.83331\n",
      "[309]\tvalidation_0-auc:0.99213\tvalidation_1-auc:0.83342\n",
      "[310]\tvalidation_0-auc:0.99216\tvalidation_1-auc:0.83345\n",
      "[311]\tvalidation_0-auc:0.99218\tvalidation_1-auc:0.83358\n",
      "[312]\tvalidation_0-auc:0.99220\tvalidation_1-auc:0.83380\n",
      "[313]\tvalidation_0-auc:0.99222\tvalidation_1-auc:0.83398\n",
      "[314]\tvalidation_0-auc:0.99224\tvalidation_1-auc:0.83403\n",
      "[315]\tvalidation_0-auc:0.99226\tvalidation_1-auc:0.83416\n",
      "[316]\tvalidation_0-auc:0.99229\tvalidation_1-auc:0.83432\n",
      "[317]\tvalidation_0-auc:0.99231\tvalidation_1-auc:0.83445\n",
      "[318]\tvalidation_0-auc:0.99234\tvalidation_1-auc:0.83460\n",
      "[319]\tvalidation_0-auc:0.99236\tvalidation_1-auc:0.83470\n",
      "[320]\tvalidation_0-auc:0.99238\tvalidation_1-auc:0.83481\n",
      "[321]\tvalidation_0-auc:0.99241\tvalidation_1-auc:0.83492\n",
      "[322]\tvalidation_0-auc:0.99242\tvalidation_1-auc:0.83500\n",
      "[323]\tvalidation_0-auc:0.99244\tvalidation_1-auc:0.83508\n",
      "[324]\tvalidation_0-auc:0.99245\tvalidation_1-auc:0.83521\n",
      "[325]\tvalidation_0-auc:0.99247\tvalidation_1-auc:0.83529\n",
      "[326]\tvalidation_0-auc:0.99249\tvalidation_1-auc:0.83537\n",
      "[327]\tvalidation_0-auc:0.99251\tvalidation_1-auc:0.83547\n",
      "[328]\tvalidation_0-auc:0.99252\tvalidation_1-auc:0.83556\n",
      "[329]\tvalidation_0-auc:0.99255\tvalidation_1-auc:0.83575\n",
      "[330]\tvalidation_0-auc:0.99256\tvalidation_1-auc:0.83586\n",
      "[331]\tvalidation_0-auc:0.99257\tvalidation_1-auc:0.83593\n",
      "[332]\tvalidation_0-auc:0.99260\tvalidation_1-auc:0.83606\n",
      "[333]\tvalidation_0-auc:0.99262\tvalidation_1-auc:0.83620\n",
      "[334]\tvalidation_0-auc:0.99264\tvalidation_1-auc:0.83632\n",
      "[335]\tvalidation_0-auc:0.99266\tvalidation_1-auc:0.83647\n",
      "[336]\tvalidation_0-auc:0.99267\tvalidation_1-auc:0.83654\n",
      "[337]\tvalidation_0-auc:0.99269\tvalidation_1-auc:0.83670\n",
      "[338]\tvalidation_0-auc:0.99271\tvalidation_1-auc:0.83683\n",
      "[339]\tvalidation_0-auc:0.99273\tvalidation_1-auc:0.83693\n",
      "[340]\tvalidation_0-auc:0.99275\tvalidation_1-auc:0.83700\n",
      "[341]\tvalidation_0-auc:0.99277\tvalidation_1-auc:0.83705\n",
      "[342]\tvalidation_0-auc:0.99278\tvalidation_1-auc:0.83718\n",
      "[343]\tvalidation_0-auc:0.99280\tvalidation_1-auc:0.83733\n",
      "[344]\tvalidation_0-auc:0.99282\tvalidation_1-auc:0.83748\n",
      "[345]\tvalidation_0-auc:0.99283\tvalidation_1-auc:0.83765\n",
      "[346]\tvalidation_0-auc:0.99285\tvalidation_1-auc:0.83767\n",
      "[347]\tvalidation_0-auc:0.99287\tvalidation_1-auc:0.83779\n",
      "[348]\tvalidation_0-auc:0.99289\tvalidation_1-auc:0.83788\n",
      "[349]\tvalidation_0-auc:0.99290\tvalidation_1-auc:0.83803\n",
      "[350]\tvalidation_0-auc:0.99292\tvalidation_1-auc:0.83814\n",
      "[351]\tvalidation_0-auc:0.99294\tvalidation_1-auc:0.83832\n",
      "[352]\tvalidation_0-auc:0.99294\tvalidation_1-auc:0.83840\n",
      "[353]\tvalidation_0-auc:0.99296\tvalidation_1-auc:0.83844\n",
      "[354]\tvalidation_0-auc:0.99298\tvalidation_1-auc:0.83848\n",
      "[355]\tvalidation_0-auc:0.99299\tvalidation_1-auc:0.83861\n",
      "[356]\tvalidation_0-auc:0.99301\tvalidation_1-auc:0.83871\n",
      "[357]\tvalidation_0-auc:0.99302\tvalidation_1-auc:0.83891\n",
      "[358]\tvalidation_0-auc:0.99304\tvalidation_1-auc:0.83890\n",
      "[359]\tvalidation_0-auc:0.99306\tvalidation_1-auc:0.83902\n",
      "[360]\tvalidation_0-auc:0.99307\tvalidation_1-auc:0.83910\n",
      "[361]\tvalidation_0-auc:0.99308\tvalidation_1-auc:0.83923\n",
      "[362]\tvalidation_0-auc:0.99309\tvalidation_1-auc:0.83935\n",
      "[363]\tvalidation_0-auc:0.99311\tvalidation_1-auc:0.83947\n",
      "[364]\tvalidation_0-auc:0.99312\tvalidation_1-auc:0.83962\n",
      "[365]\tvalidation_0-auc:0.99314\tvalidation_1-auc:0.83978\n",
      "[366]\tvalidation_0-auc:0.99316\tvalidation_1-auc:0.83989\n",
      "[367]\tvalidation_0-auc:0.99318\tvalidation_1-auc:0.83992\n",
      "[368]\tvalidation_0-auc:0.99321\tvalidation_1-auc:0.84003\n",
      "[369]\tvalidation_0-auc:0.99321\tvalidation_1-auc:0.84009\n",
      "[370]\tvalidation_0-auc:0.99322\tvalidation_1-auc:0.84013\n",
      "[371]\tvalidation_0-auc:0.99324\tvalidation_1-auc:0.84027\n",
      "[372]\tvalidation_0-auc:0.99325\tvalidation_1-auc:0.84037\n",
      "[373]\tvalidation_0-auc:0.99327\tvalidation_1-auc:0.84046\n",
      "[374]\tvalidation_0-auc:0.99328\tvalidation_1-auc:0.84055\n",
      "[375]\tvalidation_0-auc:0.99329\tvalidation_1-auc:0.84067\n",
      "[376]\tvalidation_0-auc:0.99331\tvalidation_1-auc:0.84084\n",
      "[377]\tvalidation_0-auc:0.99333\tvalidation_1-auc:0.84092\n",
      "[378]\tvalidation_0-auc:0.99334\tvalidation_1-auc:0.84106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [59]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m xgb_cl \u001B[38;5;241m=\u001B[39m xgb\u001B[38;5;241m.\u001B[39mXGBClassifier(\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# tree_method='gpu_hist', gpu_id=0,\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# max_depth = 10,\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n\u001B[0;32m     10\u001B[0m     learning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.01\u001B[39m\n\u001B[0;32m     11\u001B[0m     )\n\u001B[0;32m     12\u001B[0m eval_set  \u001B[38;5;241m=\u001B[39m [(X_train, y_train), (X_test, y_test)]\n\u001B[1;32m---> 14\u001B[0m \u001B[43mxgb_cl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m           \u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m           \u001B[49m\u001B[43meval_metric\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauc\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m          \u001B[49m\u001B[38;5;66;43;03m#  verbose=True,\u001B[39;49;00m\n\u001B[0;32m     18\u001B[0m \u001B[43m           \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m           \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m preds \u001B[38;5;241m=\u001B[39m xgb_cl\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     22\u001B[0m accuracy_score(y_test, preds)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HandsOnMachineLearning\\lib\\site-packages\\xgboost\\core.py:506\u001B[0m, in \u001B[0;36m_deprecate_positional_args.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[0;32m    505\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[1;32m--> 506\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HandsOnMachineLearning\\lib\\site-packages\\xgboost\\sklearn.py:1250\u001B[0m, in \u001B[0;36mXGBClassifier.fit\u001B[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001B[0m\n\u001B[0;32m   1230\u001B[0m model, feval, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_fit(xgb_model, eval_metric, params)\n\u001B[0;32m   1231\u001B[0m train_dmatrix, evals \u001B[38;5;241m=\u001B[39m _wrap_evaluation_matrices(\n\u001B[0;32m   1232\u001B[0m     missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing,\n\u001B[0;32m   1233\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1247\u001B[0m     label_transform\u001B[38;5;241m=\u001B[39mlabel_transform,\n\u001B[0;32m   1248\u001B[0m )\n\u001B[1;32m-> 1250\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1253\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1258\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1259\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1261\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1262\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective):\n\u001B[0;32m   1265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HandsOnMachineLearning\\lib\\site-packages\\xgboost\\training.py:188\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(params, dtrain, num_boost_round\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, evals\u001B[38;5;241m=\u001B[39m(), obj\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, feval\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    116\u001B[0m           maximize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, early_stopping_rounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, evals_result\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    117\u001B[0m           verbose_eval\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, xgb_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, callbacks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001B[39;00m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;124;03m\"\"\"Train a booster with given parameters.\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;124;03m    Booster : a trained booster model\u001B[39;00m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 188\u001B[0m     bst \u001B[38;5;241m=\u001B[39m \u001B[43m_train_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mnum_boost_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_boost_round\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxgb_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bst\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HandsOnMachineLearning\\lib\\site-packages\\xgboost\\training.py:81\u001B[0m, in \u001B[0;36m_train_internal\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callbacks\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m---> 81\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m callbacks\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HandsOnMachineLearning\\lib\\site-packages\\xgboost\\core.py:1680\u001B[0m, in \u001B[0;36mBooster.update\u001B[1;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[0;32m   1677\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_features(dtrain)\n\u001B[0;32m   1679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1680\u001B[0m     _check_call(\u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1681\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1682\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1683\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1684\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(\n",
    "    # tree_method='gpu_hist', gpu_id=0,\n",
    "    # max_depth = 10,\n",
    "    n_estimators=500,\n",
    "    learning_rate = 0.01\n",
    "    )\n",
    "eval_set  = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "xgb_cl.fit(X_train, y_train,\n",
    "           eval_set = eval_set,\n",
    "           eval_metric = 'auc',\n",
    "           verbose=True,\n",
    "           early_stopping_rounds=10,\n",
    "           )\n",
    "\n",
    "preds = xgb_cl.predict(X_test)\n",
    "accuracy_score(y_test, preds)\n",
    "xgb_cl.save_model(\"xgb.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, preds,normalize='true')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost tuning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier(\n",
    "    # tree_method='gpu_hist', gpu_id=0\n",
    "    )\n",
    "params = { 'max_depth': [3, 5, 10, 15],\n",
    "           'learning_rate': [0.01, 0.1],\n",
    "           'subsample': np.arange(0.4, 1.0, 0.2),\n",
    "           'colsample_bytree': np.arange(0.4, 1.0, 0.2),\n",
    "           'colsample_bylevel': np.arange(0.4, 1.0, 0.2),\n",
    "           'n_estimators': [100, 500, 1000]\n",
    "\n",
    "          }\n",
    "fit_params = {\n",
    "          'early_stopping_rounds': 10,\n",
    "          'eval_set' : eval_set,\n",
    "          'eval_metric': 'auc',\n",
    "          'verbose': True\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(estimator=xgb_cl,\n",
    "                         param_distributions=params,\n",
    "                         n_jobs = -1,\n",
    "                          scoring='roc_auc',\n",
    "                         n_iter=5,\n",
    "                        #  verbose=3\n",
    "                         )\n",
    "\n",
    "search = clf.fit(X_train, y_train,**fit_params)\n",
    "path = '/content/drive/MyDrive/Python/ID5059-GroupProject/'\n",
    "clf.best_estimator_.save_model(path + 'optim_xgb.json')\n",
    "preds_optimised = clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # Create a base model\n",
    "# rf = RandomForestClassifier(n_estimators= 100, max_depth = 2, oob_score = True, random_state = 10000)\n",
    "# # Create a parameter grid to conduct a grid search for hyperparameters\n",
    "# param_grid = { 'bootstrap' : [True],\n",
    "#            'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "#            'n_estimators': [100, 500, 1000]}\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 2, n_jobs = -1, verbose = 0)\n",
    "# # Fit the grid search to the data and calculate its oob score\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.6678085912433955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create a random forest classifier model with the best hypermarket fits\n",
    "rf_final = RandomForestClassifier(n_estimators = 100, max_depth = 5, n_jobs = -1, oob_score = True, random_state = 10000)\n",
    "rf_final = rf_final.fit(X_train, y_train)\n",
    "y_pred = rf_final.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "# Print AUC score\n",
    "print(f\"AUC = {score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model Tuning\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from random import  sample\n",
    "# param_grid = [\n",
    "#   {'base_estimator__max_depth' : [1, 3, 5, 7, None],\n",
    "#    'base_estimator__min_samples_split' : [0.01, 2],\n",
    "#    'base_estimator__min_samples_leaf' : [0.01, 2]\n",
    "#    }\n",
    "#  ]\n",
    "# ind = sample(range(len(X_train)), 10000)\n",
    "#\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# tree = DecisionTreeClassifier(max_features='auto',\n",
    "#                               ccp_alpha=0.0015,\n",
    "#                               )\n",
    "# bag = BaggingClassifier(base_estimator = tree,\n",
    "#                         n_estimators=1000,\n",
    "#                         max_samples=0.6,\n",
    "#                         oob_score=True,\n",
    "#                         n_jobs=-1\n",
    "#                         )\n",
    "# score = \"roc_auc\"\n",
    "# bag_clf = GridSearchCV(bag,\n",
    "#                        param_grid,\n",
    "#                        scoring=score,\n",
    "#                        n_jobs = -1)\n",
    "# bag_clf.fit(X_train.iloc[ind], y_train.iloc[ind])\n",
    "# print(\"Best parameters set found on development set:\")\n",
    "# print(bag_clf.best_params_)\n",
    "# print(\"Grid scores on development set:\")\n",
    "# means = bag_clf.cv_results_[\"mean_test_score\"]\n",
    "# stds = bag_clf.cv_results_[\"std_test_score\"]\n",
    "# for mean, std, params in zip(means, stds, bag_clf.cv_results_[\"params\"]):\n",
    "#     print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predictions\n",
    "# ind = sample(range(len(X_train)), 10000)\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# The below parameters are the optimal parameters from GridSearchCV\n",
    "tree = DecisionTreeClassifier(max_features='auto',\n",
    "                              ccp_alpha=0.0015,\n",
    "                              max_depth=7,\n",
    "                              min_samples_leaf=0.01\n",
    "                              )\n",
    "bag_clf = BaggingClassifier(base_estimator = tree,\n",
    "                        n_estimators=1000,\n",
    "                        max_samples=0.6,\n",
    "                        oob_score=True,\n",
    "                        n_jobs=-1\n",
    "                        )\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred)\n",
    "# Print AUC score\n",
    "print(f\"AUC = {score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Light Gradient Boosting Machine (LGBM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.985134\tvalid_1's auc: 0.809139\n",
      "[2]\ttraining's auc: 0.991393\tvalid_1's auc: 0.833679\n",
      "[3]\ttraining's auc: 0.993498\tvalid_1's auc: 0.835472\n",
      "[4]\ttraining's auc: 0.994444\tvalid_1's auc: 0.838359\n",
      "[5]\ttraining's auc: 0.994864\tvalid_1's auc: 0.838262\n",
      "[6]\ttraining's auc: 0.995406\tvalid_1's auc: 0.843505\n",
      "[7]\ttraining's auc: 0.995624\tvalid_1's auc: 0.843971\n",
      "[8]\ttraining's auc: 0.995734\tvalid_1's auc: 0.847717\n",
      "[9]\ttraining's auc: 0.995877\tvalid_1's auc: 0.846782\n",
      "[10]\ttraining's auc: 0.996038\tvalid_1's auc: 0.847835\n",
      "[11]\ttraining's auc: 0.996232\tvalid_1's auc: 0.846702\n",
      "[12]\ttraining's auc: 0.996296\tvalid_1's auc: 0.847761\n",
      "[13]\ttraining's auc: 0.996315\tvalid_1's auc: 0.849125\n",
      "[14]\ttraining's auc: 0.996328\tvalid_1's auc: 0.84991\n",
      "[15]\ttraining's auc: 0.996356\tvalid_1's auc: 0.851396\n",
      "[16]\ttraining's auc: 0.996376\tvalid_1's auc: 0.851852\n",
      "[17]\ttraining's auc: 0.996385\tvalid_1's auc: 0.8524\n",
      "[18]\ttraining's auc: 0.996574\tvalid_1's auc: 0.852791\n",
      "[19]\ttraining's auc: 0.99658\tvalid_1's auc: 0.854\n",
      "[20]\ttraining's auc: 0.996578\tvalid_1's auc: 0.854317\n",
      "[21]\ttraining's auc: 0.996581\tvalid_1's auc: 0.855177\n",
      "[22]\ttraining's auc: 0.996585\tvalid_1's auc: 0.855673\n",
      "[23]\ttraining's auc: 0.996589\tvalid_1's auc: 0.856593\n",
      "[24]\ttraining's auc: 0.996613\tvalid_1's auc: 0.857323\n",
      "[25]\ttraining's auc: 0.996613\tvalid_1's auc: 0.858041\n",
      "[26]\ttraining's auc: 0.996615\tvalid_1's auc: 0.858711\n",
      "[27]\ttraining's auc: 0.996623\tvalid_1's auc: 0.858931\n",
      "[28]\ttraining's auc: 0.99665\tvalid_1's auc: 0.85963\n",
      "[29]\ttraining's auc: 0.99668\tvalid_1's auc: 0.859671\n",
      "[30]\ttraining's auc: 0.996702\tvalid_1's auc: 0.860403\n",
      "[31]\ttraining's auc: 0.99671\tvalid_1's auc: 0.860744\n",
      "[32]\ttraining's auc: 0.996723\tvalid_1's auc: 0.860459\n",
      "[33]\ttraining's auc: 0.996768\tvalid_1's auc: 0.861185\n",
      "[34]\ttraining's auc: 0.996778\tvalid_1's auc: 0.861542\n",
      "[35]\ttraining's auc: 0.996773\tvalid_1's auc: 0.861501\n",
      "[36]\ttraining's auc: 0.996798\tvalid_1's auc: 0.861434\n",
      "[37]\ttraining's auc: 0.996797\tvalid_1's auc: 0.861471\n",
      "[38]\ttraining's auc: 0.996793\tvalid_1's auc: 0.861735\n",
      "[39]\ttraining's auc: 0.996794\tvalid_1's auc: 0.861742\n",
      "[40]\ttraining's auc: 0.996795\tvalid_1's auc: 0.86187\n",
      "[41]\ttraining's auc: 0.996793\tvalid_1's auc: 0.861986\n",
      "[42]\ttraining's auc: 0.996786\tvalid_1's auc: 0.862094\n",
      "[43]\ttraining's auc: 0.996794\tvalid_1's auc: 0.861962\n",
      "[44]\ttraining's auc: 0.996787\tvalid_1's auc: 0.862406\n",
      "[45]\ttraining's auc: 0.996806\tvalid_1's auc: 0.86247\n",
      "[46]\ttraining's auc: 0.996812\tvalid_1's auc: 0.862803\n",
      "[47]\ttraining's auc: 0.996803\tvalid_1's auc: 0.862447\n",
      "[48]\ttraining's auc: 0.99679\tvalid_1's auc: 0.862412\n",
      "[49]\ttraining's auc: 0.996808\tvalid_1's auc: 0.862526\n",
      "[50]\ttraining's auc: 0.996818\tvalid_1's auc: 0.86288\n",
      "[51]\ttraining's auc: 0.996821\tvalid_1's auc: 0.863135\n",
      "[52]\ttraining's auc: 0.996826\tvalid_1's auc: 0.863209\n",
      "[53]\ttraining's auc: 0.99685\tvalid_1's auc: 0.863248\n",
      "[54]\ttraining's auc: 0.996844\tvalid_1's auc: 0.863256\n",
      "[55]\ttraining's auc: 0.99685\tvalid_1's auc: 0.863369\n",
      "[56]\ttraining's auc: 0.996856\tvalid_1's auc: 0.863857\n",
      "[57]\ttraining's auc: 0.996857\tvalid_1's auc: 0.863781\n",
      "[58]\ttraining's auc: 0.996852\tvalid_1's auc: 0.863683\n",
      "[59]\ttraining's auc: 0.996852\tvalid_1's auc: 0.863865\n",
      "[60]\ttraining's auc: 0.996855\tvalid_1's auc: 0.86405\n",
      "[61]\ttraining's auc: 0.996854\tvalid_1's auc: 0.864073\n",
      "[62]\ttraining's auc: 0.996881\tvalid_1's auc: 0.864117\n",
      "[63]\ttraining's auc: 0.996891\tvalid_1's auc: 0.864577\n",
      "[64]\ttraining's auc: 0.996931\tvalid_1's auc: 0.864613\n",
      "[65]\ttraining's auc: 0.996937\tvalid_1's auc: 0.864771\n",
      "[66]\ttraining's auc: 0.996933\tvalid_1's auc: 0.865091\n",
      "[67]\ttraining's auc: 0.996938\tvalid_1's auc: 0.865268\n",
      "[68]\ttraining's auc: 0.996957\tvalid_1's auc: 0.865845\n",
      "[69]\ttraining's auc: 0.996957\tvalid_1's auc: 0.865981\n",
      "[70]\ttraining's auc: 0.99697\tvalid_1's auc: 0.866228\n",
      "[71]\ttraining's auc: 0.996974\tvalid_1's auc: 0.86621\n",
      "[72]\ttraining's auc: 0.996992\tvalid_1's auc: 0.86648\n",
      "[73]\ttraining's auc: 0.99699\tvalid_1's auc: 0.866559\n",
      "[74]\ttraining's auc: 0.997\tvalid_1's auc: 0.867088\n",
      "[75]\ttraining's auc: 0.997046\tvalid_1's auc: 0.867575\n",
      "[76]\ttraining's auc: 0.997062\tvalid_1's auc: 0.867789\n",
      "[77]\ttraining's auc: 0.997073\tvalid_1's auc: 0.868144\n",
      "[78]\ttraining's auc: 0.997069\tvalid_1's auc: 0.868182\n",
      "[79]\ttraining's auc: 0.997076\tvalid_1's auc: 0.868294\n",
      "[80]\ttraining's auc: 0.9971\tvalid_1's auc: 0.868607\n",
      "[81]\ttraining's auc: 0.99711\tvalid_1's auc: 0.86874\n",
      "[82]\ttraining's auc: 0.99711\tvalid_1's auc: 0.86887\n",
      "[83]\ttraining's auc: 0.997112\tvalid_1's auc: 0.868995\n",
      "[84]\ttraining's auc: 0.997122\tvalid_1's auc: 0.869115\n",
      "[85]\ttraining's auc: 0.997119\tvalid_1's auc: 0.869066\n",
      "[86]\ttraining's auc: 0.99713\tvalid_1's auc: 0.869282\n",
      "[87]\ttraining's auc: 0.99715\tvalid_1's auc: 0.869357\n",
      "[88]\ttraining's auc: 0.997156\tvalid_1's auc: 0.869428\n",
      "[89]\ttraining's auc: 0.997171\tvalid_1's auc: 0.869734\n",
      "[90]\ttraining's auc: 0.997171\tvalid_1's auc: 0.869696\n",
      "[91]\ttraining's auc: 0.997177\tvalid_1's auc: 0.869642\n",
      "[92]\ttraining's auc: 0.997192\tvalid_1's auc: 0.869826\n",
      "[93]\ttraining's auc: 0.997199\tvalid_1's auc: 0.870112\n",
      "[94]\ttraining's auc: 0.997208\tvalid_1's auc: 0.870312\n",
      "[95]\ttraining's auc: 0.997207\tvalid_1's auc: 0.870256\n",
      "[96]\ttraining's auc: 0.997213\tvalid_1's auc: 0.870556\n",
      "[97]\ttraining's auc: 0.997219\tvalid_1's auc: 0.870574\n",
      "[98]\ttraining's auc: 0.997228\tvalid_1's auc: 0.870704\n",
      "[99]\ttraining's auc: 0.997245\tvalid_1's auc: 0.870747\n",
      "[100]\ttraining's auc: 0.997257\tvalid_1's auc: 0.870951\n",
      "[101]\ttraining's auc: 0.997261\tvalid_1's auc: 0.871064\n",
      "[102]\ttraining's auc: 0.997265\tvalid_1's auc: 0.871045\n",
      "[103]\ttraining's auc: 0.997276\tvalid_1's auc: 0.87112\n",
      "[104]\ttraining's auc: 0.99728\tvalid_1's auc: 0.871252\n",
      "[105]\ttraining's auc: 0.997289\tvalid_1's auc: 0.871516\n",
      "[106]\ttraining's auc: 0.997285\tvalid_1's auc: 0.871593\n",
      "[107]\ttraining's auc: 0.997283\tvalid_1's auc: 0.871536\n",
      "[108]\ttraining's auc: 0.997284\tvalid_1's auc: 0.871671\n",
      "[109]\ttraining's auc: 0.997294\tvalid_1's auc: 0.871876\n",
      "[110]\ttraining's auc: 0.997314\tvalid_1's auc: 0.871904\n",
      "[111]\ttraining's auc: 0.997316\tvalid_1's auc: 0.871861\n",
      "[112]\ttraining's auc: 0.997325\tvalid_1's auc: 0.871878\n",
      "[113]\ttraining's auc: 0.997328\tvalid_1's auc: 0.871998\n",
      "[114]\ttraining's auc: 0.997329\tvalid_1's auc: 0.871988\n",
      "[115]\ttraining's auc: 0.997329\tvalid_1's auc: 0.871943\n",
      "[116]\ttraining's auc: 0.99733\tvalid_1's auc: 0.872004\n",
      "[117]\ttraining's auc: 0.997334\tvalid_1's auc: 0.872002\n",
      "[118]\ttraining's auc: 0.997339\tvalid_1's auc: 0.87222\n",
      "[119]\ttraining's auc: 0.997361\tvalid_1's auc: 0.872418\n",
      "[120]\ttraining's auc: 0.997359\tvalid_1's auc: 0.872305\n",
      "[121]\ttraining's auc: 0.997357\tvalid_1's auc: 0.872208\n",
      "[122]\ttraining's auc: 0.997359\tvalid_1's auc: 0.872175\n",
      "[123]\ttraining's auc: 0.997363\tvalid_1's auc: 0.872219\n",
      "[124]\ttraining's auc: 0.99737\tvalid_1's auc: 0.872191\n",
      "[125]\ttraining's auc: 0.997379\tvalid_1's auc: 0.872233\n",
      "[126]\ttraining's auc: 0.997383\tvalid_1's auc: 0.872366\n",
      "[127]\ttraining's auc: 0.997394\tvalid_1's auc: 0.872348\n",
      "[128]\ttraining's auc: 0.997393\tvalid_1's auc: 0.87226\n",
      "[129]\ttraining's auc: 0.997392\tvalid_1's auc: 0.872254\n",
      "[130]\ttraining's auc: 0.997394\tvalid_1's auc: 0.872284\n",
      "[131]\ttraining's auc: 0.997406\tvalid_1's auc: 0.872511\n",
      "[132]\ttraining's auc: 0.997413\tvalid_1's auc: 0.872525\n",
      "[133]\ttraining's auc: 0.997415\tvalid_1's auc: 0.872498\n",
      "[134]\ttraining's auc: 0.997426\tvalid_1's auc: 0.872847\n",
      "[135]\ttraining's auc: 0.997428\tvalid_1's auc: 0.872833\n",
      "[136]\ttraining's auc: 0.997428\tvalid_1's auc: 0.872912\n",
      "[137]\ttraining's auc: 0.997441\tvalid_1's auc: 0.873215\n",
      "[138]\ttraining's auc: 0.997459\tvalid_1's auc: 0.873375\n",
      "[139]\ttraining's auc: 0.997466\tvalid_1's auc: 0.873391\n",
      "[140]\ttraining's auc: 0.997484\tvalid_1's auc: 0.873464\n",
      "[141]\ttraining's auc: 0.997486\tvalid_1's auc: 0.873505\n",
      "[142]\ttraining's auc: 0.997503\tvalid_1's auc: 0.873645\n",
      "[143]\ttraining's auc: 0.99751\tvalid_1's auc: 0.873628\n",
      "[144]\ttraining's auc: 0.997517\tvalid_1's auc: 0.873811\n",
      "[145]\ttraining's auc: 0.997523\tvalid_1's auc: 0.873944\n",
      "[146]\ttraining's auc: 0.99753\tvalid_1's auc: 0.874118\n",
      "[147]\ttraining's auc: 0.997539\tvalid_1's auc: 0.874157\n",
      "[148]\ttraining's auc: 0.997546\tvalid_1's auc: 0.87416\n",
      "[149]\ttraining's auc: 0.997549\tvalid_1's auc: 0.874335\n",
      "[150]\ttraining's auc: 0.997552\tvalid_1's auc: 0.874381\n",
      "[151]\ttraining's auc: 0.997558\tvalid_1's auc: 0.874378\n",
      "[152]\ttraining's auc: 0.99757\tvalid_1's auc: 0.874449\n",
      "[153]\ttraining's auc: 0.997575\tvalid_1's auc: 0.874394\n",
      "[154]\ttraining's auc: 0.997579\tvalid_1's auc: 0.874443\n",
      "[155]\ttraining's auc: 0.997583\tvalid_1's auc: 0.874419\n",
      "[156]\ttraining's auc: 0.997589\tvalid_1's auc: 0.874616\n",
      "[157]\ttraining's auc: 0.997599\tvalid_1's auc: 0.874831\n",
      "[158]\ttraining's auc: 0.997606\tvalid_1's auc: 0.875104\n",
      "[159]\ttraining's auc: 0.997609\tvalid_1's auc: 0.87523\n",
      "[160]\ttraining's auc: 0.997615\tvalid_1's auc: 0.875393\n",
      "[161]\ttraining's auc: 0.997618\tvalid_1's auc: 0.875516\n",
      "[162]\ttraining's auc: 0.997622\tvalid_1's auc: 0.875595\n",
      "[163]\ttraining's auc: 0.997627\tvalid_1's auc: 0.875655\n",
      "[164]\ttraining's auc: 0.997632\tvalid_1's auc: 0.875776\n",
      "[165]\ttraining's auc: 0.99764\tvalid_1's auc: 0.875711\n",
      "[166]\ttraining's auc: 0.997644\tvalid_1's auc: 0.875711\n",
      "[167]\ttraining's auc: 0.997647\tvalid_1's auc: 0.875815\n",
      "[168]\ttraining's auc: 0.997666\tvalid_1's auc: 0.876023\n",
      "[169]\ttraining's auc: 0.997673\tvalid_1's auc: 0.876129\n",
      "[170]\ttraining's auc: 0.997677\tvalid_1's auc: 0.876252\n",
      "[171]\ttraining's auc: 0.997683\tvalid_1's auc: 0.876273\n",
      "[172]\ttraining's auc: 0.997688\tvalid_1's auc: 0.876373\n",
      "[173]\ttraining's auc: 0.997689\tvalid_1's auc: 0.876319\n",
      "[174]\ttraining's auc: 0.997692\tvalid_1's auc: 0.876357\n",
      "[175]\ttraining's auc: 0.997702\tvalid_1's auc: 0.876527\n",
      "[176]\ttraining's auc: 0.997708\tvalid_1's auc: 0.876593\n",
      "[177]\ttraining's auc: 0.99771\tvalid_1's auc: 0.876629\n",
      "[178]\ttraining's auc: 0.997711\tvalid_1's auc: 0.876688\n",
      "[179]\ttraining's auc: 0.997717\tvalid_1's auc: 0.876645\n",
      "[180]\ttraining's auc: 0.997719\tvalid_1's auc: 0.876699\n",
      "[181]\ttraining's auc: 0.997727\tvalid_1's auc: 0.87678\n",
      "[182]\ttraining's auc: 0.997732\tvalid_1's auc: 0.876793\n",
      "[183]\ttraining's auc: 0.997732\tvalid_1's auc: 0.876786\n",
      "[184]\ttraining's auc: 0.997737\tvalid_1's auc: 0.876865\n",
      "[185]\ttraining's auc: 0.997742\tvalid_1's auc: 0.87683\n",
      "[186]\ttraining's auc: 0.997745\tvalid_1's auc: 0.876917\n",
      "[187]\ttraining's auc: 0.997749\tvalid_1's auc: 0.876986\n",
      "[188]\ttraining's auc: 0.997758\tvalid_1's auc: 0.87715\n",
      "[189]\ttraining's auc: 0.997759\tvalid_1's auc: 0.877147\n",
      "[190]\ttraining's auc: 0.997762\tvalid_1's auc: 0.877261\n",
      "[191]\ttraining's auc: 0.997767\tvalid_1's auc: 0.877339\n",
      "[192]\ttraining's auc: 0.997778\tvalid_1's auc: 0.87752\n",
      "[193]\ttraining's auc: 0.997785\tvalid_1's auc: 0.877628\n",
      "[194]\ttraining's auc: 0.997786\tvalid_1's auc: 0.8776\n",
      "[195]\ttraining's auc: 0.997793\tvalid_1's auc: 0.877851\n",
      "[196]\ttraining's auc: 0.997796\tvalid_1's auc: 0.87796\n",
      "[197]\ttraining's auc: 0.997797\tvalid_1's auc: 0.877941\n",
      "[198]\ttraining's auc: 0.997802\tvalid_1's auc: 0.878063\n",
      "[199]\ttraining's auc: 0.997807\tvalid_1's auc: 0.878228\n",
      "[200]\ttraining's auc: 0.997813\tvalid_1's auc: 0.878383\n",
      "[201]\ttraining's auc: 0.997814\tvalid_1's auc: 0.878443\n",
      "[202]\ttraining's auc: 0.997816\tvalid_1's auc: 0.878438\n",
      "[203]\ttraining's auc: 0.997818\tvalid_1's auc: 0.878538\n",
      "[204]\ttraining's auc: 0.997828\tvalid_1's auc: 0.878682\n",
      "[205]\ttraining's auc: 0.997836\tvalid_1's auc: 0.878973\n",
      "[206]\ttraining's auc: 0.997841\tvalid_1's auc: 0.878994\n",
      "[207]\ttraining's auc: 0.997844\tvalid_1's auc: 0.879015\n",
      "[208]\ttraining's auc: 0.997847\tvalid_1's auc: 0.879105\n",
      "[209]\ttraining's auc: 0.997848\tvalid_1's auc: 0.879152\n",
      "[210]\ttraining's auc: 0.997853\tvalid_1's auc: 0.879276\n",
      "[211]\ttraining's auc: 0.997858\tvalid_1's auc: 0.879223\n",
      "[212]\ttraining's auc: 0.997862\tvalid_1's auc: 0.879294\n",
      "[213]\ttraining's auc: 0.997867\tvalid_1's auc: 0.879419\n",
      "[214]\ttraining's auc: 0.99787\tvalid_1's auc: 0.879427\n",
      "[215]\ttraining's auc: 0.997877\tvalid_1's auc: 0.879537\n",
      "[216]\ttraining's auc: 0.997884\tvalid_1's auc: 0.879487\n",
      "[217]\ttraining's auc: 0.997885\tvalid_1's auc: 0.879489\n",
      "[218]\ttraining's auc: 0.997893\tvalid_1's auc: 0.879791\n",
      "[219]\ttraining's auc: 0.997899\tvalid_1's auc: 0.879881\n",
      "[220]\ttraining's auc: 0.997905\tvalid_1's auc: 0.879982\n",
      "[221]\ttraining's auc: 0.997912\tvalid_1's auc: 0.880267\n",
      "[222]\ttraining's auc: 0.997914\tvalid_1's auc: 0.88028\n",
      "[223]\ttraining's auc: 0.997921\tvalid_1's auc: 0.880333\n",
      "[224]\ttraining's auc: 0.997925\tvalid_1's auc: 0.880322\n",
      "[225]\ttraining's auc: 0.997936\tvalid_1's auc: 0.880536\n",
      "[226]\ttraining's auc: 0.997938\tvalid_1's auc: 0.880613\n",
      "[227]\ttraining's auc: 0.997946\tvalid_1's auc: 0.880754\n",
      "[228]\ttraining's auc: 0.997953\tvalid_1's auc: 0.880832\n",
      "[229]\ttraining's auc: 0.997953\tvalid_1's auc: 0.880823\n",
      "[230]\ttraining's auc: 0.997957\tvalid_1's auc: 0.880903\n",
      "[231]\ttraining's auc: 0.99796\tvalid_1's auc: 0.88098\n",
      "[232]\ttraining's auc: 0.997962\tvalid_1's auc: 0.881048\n",
      "[233]\ttraining's auc: 0.997962\tvalid_1's auc: 0.881083\n",
      "[234]\ttraining's auc: 0.997969\tvalid_1's auc: 0.881217\n",
      "[235]\ttraining's auc: 0.997973\tvalid_1's auc: 0.881307\n",
      "[236]\ttraining's auc: 0.997978\tvalid_1's auc: 0.8814\n",
      "[237]\ttraining's auc: 0.997982\tvalid_1's auc: 0.88148\n",
      "[238]\ttraining's auc: 0.997989\tvalid_1's auc: 0.881676\n",
      "[239]\ttraining's auc: 0.997991\tvalid_1's auc: 0.881663\n",
      "[240]\ttraining's auc: 0.997994\tvalid_1's auc: 0.881702\n",
      "[241]\ttraining's auc: 0.997998\tvalid_1's auc: 0.881927\n",
      "[242]\ttraining's auc: 0.998005\tvalid_1's auc: 0.882083\n",
      "[243]\ttraining's auc: 0.998009\tvalid_1's auc: 0.882177\n",
      "[244]\ttraining's auc: 0.998013\tvalid_1's auc: 0.882178\n",
      "[245]\ttraining's auc: 0.998018\tvalid_1's auc: 0.882164\n",
      "[246]\ttraining's auc: 0.998022\tvalid_1's auc: 0.882292\n",
      "[247]\ttraining's auc: 0.998024\tvalid_1's auc: 0.882425\n",
      "[248]\ttraining's auc: 0.998027\tvalid_1's auc: 0.882471\n",
      "[249]\ttraining's auc: 0.998034\tvalid_1's auc: 0.882692\n",
      "[250]\ttraining's auc: 0.998035\tvalid_1's auc: 0.882703\n",
      "[251]\ttraining's auc: 0.998036\tvalid_1's auc: 0.882642\n",
      "[252]\ttraining's auc: 0.998039\tvalid_1's auc: 0.882694\n",
      "[253]\ttraining's auc: 0.998046\tvalid_1's auc: 0.882777\n",
      "[254]\ttraining's auc: 0.998052\tvalid_1's auc: 0.882814\n",
      "[255]\ttraining's auc: 0.998055\tvalid_1's auc: 0.882872\n",
      "[256]\ttraining's auc: 0.998056\tvalid_1's auc: 0.882901\n",
      "[257]\ttraining's auc: 0.998063\tvalid_1's auc: 0.883009\n",
      "[258]\ttraining's auc: 0.998067\tvalid_1's auc: 0.883082\n",
      "[259]\ttraining's auc: 0.998071\tvalid_1's auc: 0.883294\n",
      "[260]\ttraining's auc: 0.998077\tvalid_1's auc: 0.883332\n",
      "[261]\ttraining's auc: 0.99808\tvalid_1's auc: 0.883465\n",
      "[262]\ttraining's auc: 0.998085\tvalid_1's auc: 0.883595\n",
      "[263]\ttraining's auc: 0.99809\tvalid_1's auc: 0.883619\n",
      "[264]\ttraining's auc: 0.998094\tvalid_1's auc: 0.88367\n",
      "[265]\ttraining's auc: 0.998096\tvalid_1's auc: 0.883727\n",
      "[266]\ttraining's auc: 0.998098\tvalid_1's auc: 0.883693\n",
      "[267]\ttraining's auc: 0.998102\tvalid_1's auc: 0.883892\n",
      "[268]\ttraining's auc: 0.998105\tvalid_1's auc: 0.884005\n",
      "[269]\ttraining's auc: 0.998107\tvalid_1's auc: 0.884134\n",
      "[270]\ttraining's auc: 0.998112\tvalid_1's auc: 0.884189\n",
      "[271]\ttraining's auc: 0.998119\tvalid_1's auc: 0.88441\n",
      "[272]\ttraining's auc: 0.998123\tvalid_1's auc: 0.88443\n",
      "[273]\ttraining's auc: 0.998126\tvalid_1's auc: 0.884523\n",
      "[274]\ttraining's auc: 0.998129\tvalid_1's auc: 0.884522\n",
      "[275]\ttraining's auc: 0.99813\tvalid_1's auc: 0.884503\n",
      "[276]\ttraining's auc: 0.998134\tvalid_1's auc: 0.884622\n",
      "[277]\ttraining's auc: 0.998134\tvalid_1's auc: 0.884719\n",
      "[278]\ttraining's auc: 0.998139\tvalid_1's auc: 0.884777\n",
      "[279]\ttraining's auc: 0.998142\tvalid_1's auc: 0.884792\n",
      "[280]\ttraining's auc: 0.998143\tvalid_1's auc: 0.884866\n",
      "[281]\ttraining's auc: 0.998144\tvalid_1's auc: 0.884864\n",
      "[282]\ttraining's auc: 0.998148\tvalid_1's auc: 0.884968\n",
      "[283]\ttraining's auc: 0.99815\tvalid_1's auc: 0.885001\n",
      "[284]\ttraining's auc: 0.998151\tvalid_1's auc: 0.885092\n",
      "[285]\ttraining's auc: 0.998154\tvalid_1's auc: 0.885089\n",
      "[286]\ttraining's auc: 0.998162\tvalid_1's auc: 0.885217\n",
      "[287]\ttraining's auc: 0.998164\tvalid_1's auc: 0.885252\n",
      "[288]\ttraining's auc: 0.998168\tvalid_1's auc: 0.885383\n",
      "[289]\ttraining's auc: 0.998177\tvalid_1's auc: 0.885463\n",
      "[290]\ttraining's auc: 0.99818\tvalid_1's auc: 0.88553\n",
      "[291]\ttraining's auc: 0.998182\tvalid_1's auc: 0.885596\n",
      "[292]\ttraining's auc: 0.998184\tvalid_1's auc: 0.885676\n",
      "[293]\ttraining's auc: 0.998189\tvalid_1's auc: 0.885745\n",
      "[294]\ttraining's auc: 0.99819\tvalid_1's auc: 0.885746\n",
      "[295]\ttraining's auc: 0.998196\tvalid_1's auc: 0.885808\n",
      "[296]\ttraining's auc: 0.998198\tvalid_1's auc: 0.885934\n",
      "[297]\ttraining's auc: 0.9982\tvalid_1's auc: 0.886116\n",
      "[298]\ttraining's auc: 0.998206\tvalid_1's auc: 0.886296\n",
      "[299]\ttraining's auc: 0.998209\tvalid_1's auc: 0.88639\n",
      "[300]\ttraining's auc: 0.998213\tvalid_1's auc: 0.886482\n",
      "[301]\ttraining's auc: 0.998218\tvalid_1's auc: 0.886604\n",
      "[302]\ttraining's auc: 0.998221\tvalid_1's auc: 0.886596\n",
      "[303]\ttraining's auc: 0.998227\tvalid_1's auc: 0.886679\n",
      "[304]\ttraining's auc: 0.998229\tvalid_1's auc: 0.886737\n",
      "[305]\ttraining's auc: 0.998233\tvalid_1's auc: 0.886835\n",
      "[306]\ttraining's auc: 0.998235\tvalid_1's auc: 0.886901\n",
      "[307]\ttraining's auc: 0.99824\tvalid_1's auc: 0.887037\n",
      "[308]\ttraining's auc: 0.998242\tvalid_1's auc: 0.887225\n",
      "[309]\ttraining's auc: 0.998243\tvalid_1's auc: 0.887315\n",
      "[310]\ttraining's auc: 0.998246\tvalid_1's auc: 0.887299\n",
      "[311]\ttraining's auc: 0.998249\tvalid_1's auc: 0.887349\n",
      "[312]\ttraining's auc: 0.998257\tvalid_1's auc: 0.887495\n",
      "[313]\ttraining's auc: 0.998258\tvalid_1's auc: 0.887492\n",
      "[314]\ttraining's auc: 0.99826\tvalid_1's auc: 0.887552\n",
      "[315]\ttraining's auc: 0.998261\tvalid_1's auc: 0.887538\n",
      "[316]\ttraining's auc: 0.998267\tvalid_1's auc: 0.887593\n",
      "[317]\ttraining's auc: 0.998268\tvalid_1's auc: 0.887654\n",
      "[318]\ttraining's auc: 0.998272\tvalid_1's auc: 0.887745\n",
      "[319]\ttraining's auc: 0.998279\tvalid_1's auc: 0.888038\n",
      "[320]\ttraining's auc: 0.998281\tvalid_1's auc: 0.888053\n",
      "[321]\ttraining's auc: 0.998285\tvalid_1's auc: 0.88819\n",
      "[322]\ttraining's auc: 0.998287\tvalid_1's auc: 0.888194\n",
      "[323]\ttraining's auc: 0.99829\tvalid_1's auc: 0.888307\n",
      "[324]\ttraining's auc: 0.998292\tvalid_1's auc: 0.888413\n",
      "[325]\ttraining's auc: 0.998297\tvalid_1's auc: 0.888485\n",
      "[326]\ttraining's auc: 0.998302\tvalid_1's auc: 0.888645\n",
      "[327]\ttraining's auc: 0.998303\tvalid_1's auc: 0.888685\n",
      "[328]\ttraining's auc: 0.998305\tvalid_1's auc: 0.888666\n",
      "[329]\ttraining's auc: 0.998308\tvalid_1's auc: 0.888761\n",
      "[330]\ttraining's auc: 0.998313\tvalid_1's auc: 0.888957\n",
      "[331]\ttraining's auc: 0.998316\tvalid_1's auc: 0.88897\n",
      "[332]\ttraining's auc: 0.998318\tvalid_1's auc: 0.889006\n",
      "[333]\ttraining's auc: 0.99832\tvalid_1's auc: 0.889037\n",
      "[334]\ttraining's auc: 0.998323\tvalid_1's auc: 0.889172\n",
      "[335]\ttraining's auc: 0.998326\tvalid_1's auc: 0.889163\n",
      "[336]\ttraining's auc: 0.998328\tvalid_1's auc: 0.889214\n",
      "[337]\ttraining's auc: 0.998334\tvalid_1's auc: 0.88924\n",
      "[338]\ttraining's auc: 0.998338\tvalid_1's auc: 0.889376\n",
      "[339]\ttraining's auc: 0.99834\tvalid_1's auc: 0.889411\n",
      "[340]\ttraining's auc: 0.998343\tvalid_1's auc: 0.88955\n",
      "[341]\ttraining's auc: 0.998347\tvalid_1's auc: 0.889587\n",
      "[342]\ttraining's auc: 0.998352\tvalid_1's auc: 0.889851\n",
      "[343]\ttraining's auc: 0.998355\tvalid_1's auc: 0.889906\n",
      "[344]\ttraining's auc: 0.998356\tvalid_1's auc: 0.889975\n",
      "[345]\ttraining's auc: 0.998358\tvalid_1's auc: 0.890035\n",
      "[346]\ttraining's auc: 0.99836\tvalid_1's auc: 0.890069\n",
      "[347]\ttraining's auc: 0.998363\tvalid_1's auc: 0.890056\n",
      "[348]\ttraining's auc: 0.998367\tvalid_1's auc: 0.890119\n",
      "[349]\ttraining's auc: 0.998369\tvalid_1's auc: 0.890155\n",
      "[350]\ttraining's auc: 0.998372\tvalid_1's auc: 0.890209\n",
      "[351]\ttraining's auc: 0.998375\tvalid_1's auc: 0.890246\n",
      "[352]\ttraining's auc: 0.998379\tvalid_1's auc: 0.890317\n",
      "[353]\ttraining's auc: 0.998385\tvalid_1's auc: 0.890588\n",
      "[354]\ttraining's auc: 0.998387\tvalid_1's auc: 0.890595\n",
      "[355]\ttraining's auc: 0.998391\tvalid_1's auc: 0.890607\n",
      "[356]\ttraining's auc: 0.998393\tvalid_1's auc: 0.890807\n",
      "[357]\ttraining's auc: 0.998397\tvalid_1's auc: 0.890783\n",
      "[358]\ttraining's auc: 0.9984\tvalid_1's auc: 0.890845\n",
      "[359]\ttraining's auc: 0.998403\tvalid_1's auc: 0.890916\n",
      "[360]\ttraining's auc: 0.998406\tvalid_1's auc: 0.890993\n",
      "[361]\ttraining's auc: 0.99841\tvalid_1's auc: 0.89116\n",
      "[362]\ttraining's auc: 0.998413\tvalid_1's auc: 0.891209\n",
      "[363]\ttraining's auc: 0.998416\tvalid_1's auc: 0.891279\n",
      "[364]\ttraining's auc: 0.998419\tvalid_1's auc: 0.891381\n",
      "[365]\ttraining's auc: 0.998422\tvalid_1's auc: 0.891424\n",
      "[366]\ttraining's auc: 0.998424\tvalid_1's auc: 0.891514\n",
      "[367]\ttraining's auc: 0.998428\tvalid_1's auc: 0.891618\n",
      "[368]\ttraining's auc: 0.998431\tvalid_1's auc: 0.89175\n",
      "[369]\ttraining's auc: 0.998433\tvalid_1's auc: 0.891768\n",
      "[370]\ttraining's auc: 0.998436\tvalid_1's auc: 0.891785\n",
      "[371]\ttraining's auc: 0.99844\tvalid_1's auc: 0.891893\n",
      "[372]\ttraining's auc: 0.998443\tvalid_1's auc: 0.891923\n",
      "[373]\ttraining's auc: 0.998445\tvalid_1's auc: 0.89195\n",
      "[374]\ttraining's auc: 0.998449\tvalid_1's auc: 0.892047\n",
      "[375]\ttraining's auc: 0.998451\tvalid_1's auc: 0.892091\n",
      "[376]\ttraining's auc: 0.998453\tvalid_1's auc: 0.892113\n",
      "[377]\ttraining's auc: 0.998456\tvalid_1's auc: 0.892272\n",
      "[378]\ttraining's auc: 0.998457\tvalid_1's auc: 0.892219\n",
      "[379]\ttraining's auc: 0.998459\tvalid_1's auc: 0.892272\n",
      "[380]\ttraining's auc: 0.998463\tvalid_1's auc: 0.892382\n",
      "[381]\ttraining's auc: 0.998465\tvalid_1's auc: 0.892439\n",
      "[382]\ttraining's auc: 0.998468\tvalid_1's auc: 0.892543\n",
      "[383]\ttraining's auc: 0.998471\tvalid_1's auc: 0.892633\n",
      "[384]\ttraining's auc: 0.998473\tvalid_1's auc: 0.892676\n",
      "[385]\ttraining's auc: 0.998475\tvalid_1's auc: 0.892807\n",
      "[386]\ttraining's auc: 0.998476\tvalid_1's auc: 0.892816\n",
      "[387]\ttraining's auc: 0.998479\tvalid_1's auc: 0.892887\n",
      "[388]\ttraining's auc: 0.998482\tvalid_1's auc: 0.892956\n",
      "[389]\ttraining's auc: 0.998484\tvalid_1's auc: 0.89297\n",
      "[390]\ttraining's auc: 0.998487\tvalid_1's auc: 0.89306\n",
      "[391]\ttraining's auc: 0.998489\tvalid_1's auc: 0.893131\n",
      "[392]\ttraining's auc: 0.998491\tvalid_1's auc: 0.893209\n",
      "[393]\ttraining's auc: 0.998493\tvalid_1's auc: 0.893262\n",
      "[394]\ttraining's auc: 0.998496\tvalid_1's auc: 0.893431\n",
      "[395]\ttraining's auc: 0.998498\tvalid_1's auc: 0.893521\n",
      "[396]\ttraining's auc: 0.9985\tvalid_1's auc: 0.893612\n",
      "[397]\ttraining's auc: 0.998503\tvalid_1's auc: 0.8937\n",
      "[398]\ttraining's auc: 0.998505\tvalid_1's auc: 0.893738\n",
      "[399]\ttraining's auc: 0.998507\tvalid_1's auc: 0.893758\n",
      "[400]\ttraining's auc: 0.998509\tvalid_1's auc: 0.893775\n",
      "[401]\ttraining's auc: 0.99851\tvalid_1's auc: 0.893802\n",
      "[402]\ttraining's auc: 0.998512\tvalid_1's auc: 0.893855\n",
      "[403]\ttraining's auc: 0.998514\tvalid_1's auc: 0.893891\n",
      "[404]\ttraining's auc: 0.998517\tvalid_1's auc: 0.894012\n",
      "[405]\ttraining's auc: 0.998519\tvalid_1's auc: 0.894047\n",
      "[406]\ttraining's auc: 0.998521\tvalid_1's auc: 0.894087\n",
      "[407]\ttraining's auc: 0.998522\tvalid_1's auc: 0.894106\n",
      "[408]\ttraining's auc: 0.998525\tvalid_1's auc: 0.894142\n",
      "[409]\ttraining's auc: 0.998529\tvalid_1's auc: 0.89428\n",
      "[410]\ttraining's auc: 0.998532\tvalid_1's auc: 0.894333\n",
      "[411]\ttraining's auc: 0.998534\tvalid_1's auc: 0.894355\n",
      "[412]\ttraining's auc: 0.998537\tvalid_1's auc: 0.894422\n",
      "[413]\ttraining's auc: 0.99854\tvalid_1's auc: 0.894503\n",
      "[414]\ttraining's auc: 0.998543\tvalid_1's auc: 0.894574\n",
      "[415]\ttraining's auc: 0.998545\tvalid_1's auc: 0.894639\n",
      "[416]\ttraining's auc: 0.998547\tvalid_1's auc: 0.894713\n",
      "[417]\ttraining's auc: 0.998549\tvalid_1's auc: 0.894765\n",
      "[418]\ttraining's auc: 0.998552\tvalid_1's auc: 0.894818\n",
      "[419]\ttraining's auc: 0.998554\tvalid_1's auc: 0.894884\n",
      "[420]\ttraining's auc: 0.998556\tvalid_1's auc: 0.894953\n",
      "[421]\ttraining's auc: 0.998558\tvalid_1's auc: 0.895022\n",
      "[422]\ttraining's auc: 0.99856\tvalid_1's auc: 0.895106\n",
      "[423]\ttraining's auc: 0.998563\tvalid_1's auc: 0.895183\n",
      "[424]\ttraining's auc: 0.998565\tvalid_1's auc: 0.895219\n",
      "[425]\ttraining's auc: 0.998568\tvalid_1's auc: 0.895267\n",
      "[426]\ttraining's auc: 0.99857\tvalid_1's auc: 0.89536\n",
      "[427]\ttraining's auc: 0.998572\tvalid_1's auc: 0.895487\n",
      "[428]\ttraining's auc: 0.998575\tvalid_1's auc: 0.895609\n",
      "[429]\ttraining's auc: 0.998578\tvalid_1's auc: 0.895673\n",
      "[430]\ttraining's auc: 0.998579\tvalid_1's auc: 0.895714\n",
      "[431]\ttraining's auc: 0.998582\tvalid_1's auc: 0.895784\n",
      "[432]\ttraining's auc: 0.998584\tvalid_1's auc: 0.895867\n",
      "[433]\ttraining's auc: 0.998588\tvalid_1's auc: 0.89604\n",
      "[434]\ttraining's auc: 0.99859\tvalid_1's auc: 0.896043\n",
      "[435]\ttraining's auc: 0.998593\tvalid_1's auc: 0.896095\n",
      "[436]\ttraining's auc: 0.998597\tvalid_1's auc: 0.896262\n",
      "[437]\ttraining's auc: 0.998599\tvalid_1's auc: 0.896332\n",
      "[438]\ttraining's auc: 0.998602\tvalid_1's auc: 0.896384\n",
      "[439]\ttraining's auc: 0.998604\tvalid_1's auc: 0.896441\n",
      "[440]\ttraining's auc: 0.998607\tvalid_1's auc: 0.896588\n",
      "[441]\ttraining's auc: 0.998609\tvalid_1's auc: 0.896711\n",
      "[442]\ttraining's auc: 0.998613\tvalid_1's auc: 0.896891\n",
      "[443]\ttraining's auc: 0.998616\tvalid_1's auc: 0.897001\n",
      "[444]\ttraining's auc: 0.998619\tvalid_1's auc: 0.897134\n",
      "[445]\ttraining's auc: 0.998621\tvalid_1's auc: 0.897161\n",
      "[446]\ttraining's auc: 0.998623\tvalid_1's auc: 0.897253\n",
      "[447]\ttraining's auc: 0.998625\tvalid_1's auc: 0.897298\n",
      "[448]\ttraining's auc: 0.998628\tvalid_1's auc: 0.897431\n",
      "[449]\ttraining's auc: 0.998631\tvalid_1's auc: 0.897479\n",
      "[450]\ttraining's auc: 0.998634\tvalid_1's auc: 0.897573\n",
      "[451]\ttraining's auc: 0.998636\tvalid_1's auc: 0.89762\n",
      "[452]\ttraining's auc: 0.998639\tvalid_1's auc: 0.897751\n",
      "[453]\ttraining's auc: 0.998642\tvalid_1's auc: 0.897806\n",
      "[454]\ttraining's auc: 0.998644\tvalid_1's auc: 0.89784\n",
      "[455]\ttraining's auc: 0.998646\tvalid_1's auc: 0.897917\n",
      "[456]\ttraining's auc: 0.998649\tvalid_1's auc: 0.897945\n",
      "[457]\ttraining's auc: 0.998652\tvalid_1's auc: 0.898045\n",
      "[458]\ttraining's auc: 0.998655\tvalid_1's auc: 0.898137\n",
      "[459]\ttraining's auc: 0.998657\tvalid_1's auc: 0.898181\n",
      "[460]\ttraining's auc: 0.998659\tvalid_1's auc: 0.898215\n",
      "[461]\ttraining's auc: 0.998662\tvalid_1's auc: 0.898269\n",
      "[462]\ttraining's auc: 0.998664\tvalid_1's auc: 0.898329\n",
      "[463]\ttraining's auc: 0.998666\tvalid_1's auc: 0.898442\n",
      "[464]\ttraining's auc: 0.998668\tvalid_1's auc: 0.898468\n",
      "[465]\ttraining's auc: 0.99867\tvalid_1's auc: 0.898501\n",
      "[466]\ttraining's auc: 0.998672\tvalid_1's auc: 0.898586\n",
      "[467]\ttraining's auc: 0.998674\tvalid_1's auc: 0.898646\n",
      "[468]\ttraining's auc: 0.998675\tvalid_1's auc: 0.898719\n",
      "[469]\ttraining's auc: 0.998677\tvalid_1's auc: 0.898779\n",
      "[470]\ttraining's auc: 0.998679\tvalid_1's auc: 0.898865\n",
      "[471]\ttraining's auc: 0.998684\tvalid_1's auc: 0.899128\n",
      "[472]\ttraining's auc: 0.998685\tvalid_1's auc: 0.899126\n",
      "[473]\ttraining's auc: 0.998687\tvalid_1's auc: 0.899216\n",
      "[474]\ttraining's auc: 0.99869\tvalid_1's auc: 0.899357\n",
      "[475]\ttraining's auc: 0.998692\tvalid_1's auc: 0.899408\n",
      "[476]\ttraining's auc: 0.998694\tvalid_1's auc: 0.899433\n",
      "[477]\ttraining's auc: 0.998697\tvalid_1's auc: 0.899512\n",
      "[478]\ttraining's auc: 0.998699\tvalid_1's auc: 0.899615\n",
      "[479]\ttraining's auc: 0.998701\tvalid_1's auc: 0.899668\n",
      "[480]\ttraining's auc: 0.998704\tvalid_1's auc: 0.899706\n",
      "[481]\ttraining's auc: 0.998708\tvalid_1's auc: 0.899754\n",
      "[482]\ttraining's auc: 0.998709\tvalid_1's auc: 0.899825\n",
      "[483]\ttraining's auc: 0.998712\tvalid_1's auc: 0.899869\n",
      "[484]\ttraining's auc: 0.998714\tvalid_1's auc: 0.899888\n",
      "[485]\ttraining's auc: 0.998715\tvalid_1's auc: 0.899934\n",
      "[486]\ttraining's auc: 0.998718\tvalid_1's auc: 0.900085\n",
      "[487]\ttraining's auc: 0.998721\tvalid_1's auc: 0.900239\n",
      "[488]\ttraining's auc: 0.998725\tvalid_1's auc: 0.900421\n",
      "[489]\ttraining's auc: 0.998728\tvalid_1's auc: 0.900522\n",
      "[490]\ttraining's auc: 0.99873\tvalid_1's auc: 0.900575\n",
      "[491]\ttraining's auc: 0.998732\tvalid_1's auc: 0.900634\n",
      "[492]\ttraining's auc: 0.998734\tvalid_1's auc: 0.900744\n",
      "[493]\ttraining's auc: 0.998736\tvalid_1's auc: 0.900813\n",
      "[494]\ttraining's auc: 0.998737\tvalid_1's auc: 0.900857\n",
      "[495]\ttraining's auc: 0.99874\tvalid_1's auc: 0.900916\n",
      "[496]\ttraining's auc: 0.998744\tvalid_1's auc: 0.901024\n",
      "[497]\ttraining's auc: 0.998746\tvalid_1's auc: 0.901048\n",
      "[498]\ttraining's auc: 0.998748\tvalid_1's auc: 0.90106\n",
      "[499]\ttraining's auc: 0.99875\tvalid_1's auc: 0.901091\n",
      "[500]\ttraining's auc: 0.998753\tvalid_1's auc: 0.901226\n",
      "[501]\ttraining's auc: 0.998756\tvalid_1's auc: 0.901273\n",
      "[502]\ttraining's auc: 0.998758\tvalid_1's auc: 0.901302\n",
      "[503]\ttraining's auc: 0.99876\tvalid_1's auc: 0.901333\n",
      "[504]\ttraining's auc: 0.998763\tvalid_1's auc: 0.901337\n",
      "[505]\ttraining's auc: 0.998765\tvalid_1's auc: 0.901472\n",
      "[506]\ttraining's auc: 0.998767\tvalid_1's auc: 0.901506\n",
      "[507]\ttraining's auc: 0.998769\tvalid_1's auc: 0.901579\n",
      "[508]\ttraining's auc: 0.998772\tvalid_1's auc: 0.901591\n",
      "[509]\ttraining's auc: 0.998774\tvalid_1's auc: 0.901617\n",
      "[510]\ttraining's auc: 0.998776\tvalid_1's auc: 0.901642\n",
      "[511]\ttraining's auc: 0.99878\tvalid_1's auc: 0.901804\n",
      "[512]\ttraining's auc: 0.998781\tvalid_1's auc: 0.901871\n",
      "[513]\ttraining's auc: 0.998783\tvalid_1's auc: 0.901905\n",
      "[514]\ttraining's auc: 0.998785\tvalid_1's auc: 0.901983\n",
      "[515]\ttraining's auc: 0.998787\tvalid_1's auc: 0.902072\n",
      "[516]\ttraining's auc: 0.998789\tvalid_1's auc: 0.902124\n",
      "[517]\ttraining's auc: 0.998791\tvalid_1's auc: 0.90216\n",
      "[518]\ttraining's auc: 0.998794\tvalid_1's auc: 0.902229\n",
      "[519]\ttraining's auc: 0.998796\tvalid_1's auc: 0.902274\n",
      "[520]\ttraining's auc: 0.998798\tvalid_1's auc: 0.902312\n",
      "[521]\ttraining's auc: 0.998802\tvalid_1's auc: 0.902478\n",
      "[522]\ttraining's auc: 0.998804\tvalid_1's auc: 0.902511\n",
      "[523]\ttraining's auc: 0.998806\tvalid_1's auc: 0.902608\n",
      "[524]\ttraining's auc: 0.998808\tvalid_1's auc: 0.902688\n",
      "[525]\ttraining's auc: 0.99881\tvalid_1's auc: 0.902737\n",
      "[526]\ttraining's auc: 0.998813\tvalid_1's auc: 0.902828\n",
      "[527]\ttraining's auc: 0.998816\tvalid_1's auc: 0.902908\n",
      "[528]\ttraining's auc: 0.998819\tvalid_1's auc: 0.902961\n",
      "[529]\ttraining's auc: 0.998821\tvalid_1's auc: 0.903022\n",
      "[530]\ttraining's auc: 0.998823\tvalid_1's auc: 0.90314\n",
      "[531]\ttraining's auc: 0.998827\tvalid_1's auc: 0.903233\n",
      "[532]\ttraining's auc: 0.998829\tvalid_1's auc: 0.903291\n",
      "[533]\ttraining's auc: 0.998832\tvalid_1's auc: 0.903435\n",
      "[534]\ttraining's auc: 0.998836\tvalid_1's auc: 0.903498\n",
      "[535]\ttraining's auc: 0.998838\tvalid_1's auc: 0.903559\n",
      "[536]\ttraining's auc: 0.99884\tvalid_1's auc: 0.903582\n",
      "[537]\ttraining's auc: 0.998841\tvalid_1's auc: 0.9036\n",
      "[538]\ttraining's auc: 0.998844\tvalid_1's auc: 0.903693\n",
      "[539]\ttraining's auc: 0.998846\tvalid_1's auc: 0.903775\n",
      "[540]\ttraining's auc: 0.998849\tvalid_1's auc: 0.903835\n",
      "[541]\ttraining's auc: 0.998851\tvalid_1's auc: 0.903926\n",
      "[542]\ttraining's auc: 0.998855\tvalid_1's auc: 0.90402\n",
      "[543]\ttraining's auc: 0.998857\tvalid_1's auc: 0.904105\n",
      "[544]\ttraining's auc: 0.998859\tvalid_1's auc: 0.90421\n",
      "[545]\ttraining's auc: 0.998863\tvalid_1's auc: 0.904327\n",
      "[546]\ttraining's auc: 0.998865\tvalid_1's auc: 0.904406\n",
      "[547]\ttraining's auc: 0.998868\tvalid_1's auc: 0.904514\n",
      "[548]\ttraining's auc: 0.998872\tvalid_1's auc: 0.904576\n",
      "[549]\ttraining's auc: 0.998875\tvalid_1's auc: 0.904737\n",
      "[550]\ttraining's auc: 0.998878\tvalid_1's auc: 0.90482\n",
      "[551]\ttraining's auc: 0.99888\tvalid_1's auc: 0.904865\n",
      "[552]\ttraining's auc: 0.998883\tvalid_1's auc: 0.904995\n",
      "[553]\ttraining's auc: 0.998885\tvalid_1's auc: 0.904989\n",
      "[554]\ttraining's auc: 0.998888\tvalid_1's auc: 0.905125\n",
      "[555]\ttraining's auc: 0.99889\tvalid_1's auc: 0.905253\n",
      "[556]\ttraining's auc: 0.998892\tvalid_1's auc: 0.905312\n",
      "[557]\ttraining's auc: 0.998894\tvalid_1's auc: 0.905453\n",
      "[558]\ttraining's auc: 0.998897\tvalid_1's auc: 0.905564\n",
      "[559]\ttraining's auc: 0.998901\tvalid_1's auc: 0.905673\n",
      "[560]\ttraining's auc: 0.998903\tvalid_1's auc: 0.905707\n",
      "[561]\ttraining's auc: 0.998905\tvalid_1's auc: 0.905818\n",
      "[562]\ttraining's auc: 0.998909\tvalid_1's auc: 0.905966\n",
      "[563]\ttraining's auc: 0.998911\tvalid_1's auc: 0.906045\n",
      "[564]\ttraining's auc: 0.998915\tvalid_1's auc: 0.906182\n",
      "[565]\ttraining's auc: 0.998917\tvalid_1's auc: 0.90631\n",
      "[566]\ttraining's auc: 0.998919\tvalid_1's auc: 0.906373\n",
      "[567]\ttraining's auc: 0.998922\tvalid_1's auc: 0.906509\n",
      "[568]\ttraining's auc: 0.998924\tvalid_1's auc: 0.906582\n",
      "[569]\ttraining's auc: 0.998927\tvalid_1's auc: 0.906651\n",
      "[570]\ttraining's auc: 0.998931\tvalid_1's auc: 0.906768\n",
      "[571]\ttraining's auc: 0.998934\tvalid_1's auc: 0.906859\n",
      "[572]\ttraining's auc: 0.998936\tvalid_1's auc: 0.90694\n",
      "[573]\ttraining's auc: 0.998938\tvalid_1's auc: 0.907034\n",
      "[574]\ttraining's auc: 0.998941\tvalid_1's auc: 0.907176\n",
      "[575]\ttraining's auc: 0.998943\tvalid_1's auc: 0.907215\n",
      "[576]\ttraining's auc: 0.998945\tvalid_1's auc: 0.907264\n",
      "[577]\ttraining's auc: 0.998948\tvalid_1's auc: 0.907385\n",
      "[578]\ttraining's auc: 0.998952\tvalid_1's auc: 0.907531\n",
      "[579]\ttraining's auc: 0.998956\tvalid_1's auc: 0.90768\n",
      "[580]\ttraining's auc: 0.998958\tvalid_1's auc: 0.907717\n",
      "[581]\ttraining's auc: 0.998961\tvalid_1's auc: 0.907891\n",
      "[582]\ttraining's auc: 0.998964\tvalid_1's auc: 0.907948\n",
      "[583]\ttraining's auc: 0.998966\tvalid_1's auc: 0.907991\n",
      "[584]\ttraining's auc: 0.998968\tvalid_1's auc: 0.908067\n",
      "[585]\ttraining's auc: 0.998971\tvalid_1's auc: 0.908116\n",
      "[586]\ttraining's auc: 0.998973\tvalid_1's auc: 0.908164\n",
      "[587]\ttraining's auc: 0.998975\tvalid_1's auc: 0.90821\n",
      "[588]\ttraining's auc: 0.998978\tvalid_1's auc: 0.908243\n",
      "[589]\ttraining's auc: 0.998979\tvalid_1's auc: 0.908321\n",
      "[590]\ttraining's auc: 0.998982\tvalid_1's auc: 0.908459\n",
      "[591]\ttraining's auc: 0.998985\tvalid_1's auc: 0.908602\n",
      "[592]\ttraining's auc: 0.998987\tvalid_1's auc: 0.908698\n",
      "[593]\ttraining's auc: 0.99899\tvalid_1's auc: 0.908803\n",
      "[594]\ttraining's auc: 0.998992\tvalid_1's auc: 0.908842\n",
      "[595]\ttraining's auc: 0.998995\tvalid_1's auc: 0.908983\n",
      "[596]\ttraining's auc: 0.998998\tvalid_1's auc: 0.909038\n",
      "[597]\ttraining's auc: 0.999\tvalid_1's auc: 0.909115\n",
      "[598]\ttraining's auc: 0.999002\tvalid_1's auc: 0.909202\n",
      "[599]\ttraining's auc: 0.999004\tvalid_1's auc: 0.909251\n",
      "[600]\ttraining's auc: 0.999007\tvalid_1's auc: 0.909329\n",
      "[601]\ttraining's auc: 0.999009\tvalid_1's auc: 0.909408\n",
      "[602]\ttraining's auc: 0.999011\tvalid_1's auc: 0.909504\n",
      "[603]\ttraining's auc: 0.999012\tvalid_1's auc: 0.909532\n",
      "[604]\ttraining's auc: 0.999015\tvalid_1's auc: 0.9096\n",
      "[605]\ttraining's auc: 0.999017\tvalid_1's auc: 0.909652\n",
      "[606]\ttraining's auc: 0.99902\tvalid_1's auc: 0.909726\n",
      "[607]\ttraining's auc: 0.999022\tvalid_1's auc: 0.909764\n",
      "[608]\ttraining's auc: 0.999025\tvalid_1's auc: 0.909861\n",
      "[609]\ttraining's auc: 0.999028\tvalid_1's auc: 0.909894\n",
      "[610]\ttraining's auc: 0.999031\tvalid_1's auc: 0.910067\n",
      "[611]\ttraining's auc: 0.999033\tvalid_1's auc: 0.910114\n",
      "[612]\ttraining's auc: 0.999035\tvalid_1's auc: 0.910141\n",
      "[613]\ttraining's auc: 0.999037\tvalid_1's auc: 0.910171\n",
      "[614]\ttraining's auc: 0.999039\tvalid_1's auc: 0.910289\n",
      "[615]\ttraining's auc: 0.999043\tvalid_1's auc: 0.910384\n",
      "[616]\ttraining's auc: 0.999045\tvalid_1's auc: 0.910519\n",
      "[617]\ttraining's auc: 0.999048\tvalid_1's auc: 0.910754\n",
      "[618]\ttraining's auc: 0.999052\tvalid_1's auc: 0.910933\n",
      "[619]\ttraining's auc: 0.999054\tvalid_1's auc: 0.911012\n",
      "[620]\ttraining's auc: 0.999055\tvalid_1's auc: 0.911037\n",
      "[621]\ttraining's auc: 0.999057\tvalid_1's auc: 0.911111\n",
      "[622]\ttraining's auc: 0.99906\tvalid_1's auc: 0.911186\n",
      "[623]\ttraining's auc: 0.999063\tvalid_1's auc: 0.911324\n",
      "[624]\ttraining's auc: 0.999065\tvalid_1's auc: 0.91143\n",
      "[625]\ttraining's auc: 0.999068\tvalid_1's auc: 0.911508\n",
      "[626]\ttraining's auc: 0.99907\tvalid_1's auc: 0.91156\n",
      "[627]\ttraining's auc: 0.999073\tvalid_1's auc: 0.911637\n",
      "[628]\ttraining's auc: 0.999075\tvalid_1's auc: 0.911716\n",
      "[629]\ttraining's auc: 0.999078\tvalid_1's auc: 0.911805\n",
      "[630]\ttraining's auc: 0.999081\tvalid_1's auc: 0.911927\n",
      "[631]\ttraining's auc: 0.999083\tvalid_1's auc: 0.911949\n",
      "[632]\ttraining's auc: 0.999085\tvalid_1's auc: 0.911976\n",
      "[633]\ttraining's auc: 0.999088\tvalid_1's auc: 0.912045\n",
      "[634]\ttraining's auc: 0.999091\tvalid_1's auc: 0.912167\n",
      "[635]\ttraining's auc: 0.999093\tvalid_1's auc: 0.912233\n",
      "[636]\ttraining's auc: 0.999095\tvalid_1's auc: 0.912321\n",
      "[637]\ttraining's auc: 0.999097\tvalid_1's auc: 0.912403\n",
      "[638]\ttraining's auc: 0.999101\tvalid_1's auc: 0.912509\n",
      "[639]\ttraining's auc: 0.999104\tvalid_1's auc: 0.912552\n",
      "[640]\ttraining's auc: 0.999106\tvalid_1's auc: 0.912602\n",
      "[641]\ttraining's auc: 0.999108\tvalid_1's auc: 0.91266\n",
      "[642]\ttraining's auc: 0.999109\tvalid_1's auc: 0.912662\n",
      "[643]\ttraining's auc: 0.999111\tvalid_1's auc: 0.912719\n",
      "[644]\ttraining's auc: 0.999113\tvalid_1's auc: 0.912753\n",
      "[645]\ttraining's auc: 0.999115\tvalid_1's auc: 0.912817\n",
      "[646]\ttraining's auc: 0.999117\tvalid_1's auc: 0.912931\n",
      "[647]\ttraining's auc: 0.999119\tvalid_1's auc: 0.912966\n",
      "[648]\ttraining's auc: 0.999121\tvalid_1's auc: 0.912997\n",
      "[649]\ttraining's auc: 0.999123\tvalid_1's auc: 0.913063\n",
      "[650]\ttraining's auc: 0.999126\tvalid_1's auc: 0.913225\n",
      "[651]\ttraining's auc: 0.999129\tvalid_1's auc: 0.913335\n",
      "[652]\ttraining's auc: 0.999131\tvalid_1's auc: 0.913357\n",
      "[653]\ttraining's auc: 0.999134\tvalid_1's auc: 0.913437\n",
      "[654]\ttraining's auc: 0.999137\tvalid_1's auc: 0.913657\n",
      "[655]\ttraining's auc: 0.99914\tvalid_1's auc: 0.913712\n",
      "[656]\ttraining's auc: 0.999142\tvalid_1's auc: 0.913812\n",
      "[657]\ttraining's auc: 0.999144\tvalid_1's auc: 0.913879\n",
      "[658]\ttraining's auc: 0.999146\tvalid_1's auc: 0.913915\n",
      "[659]\ttraining's auc: 0.999149\tvalid_1's auc: 0.91402\n",
      "[660]\ttraining's auc: 0.999151\tvalid_1's auc: 0.914106\n",
      "[661]\ttraining's auc: 0.999154\tvalid_1's auc: 0.914151\n",
      "[662]\ttraining's auc: 0.999156\tvalid_1's auc: 0.914195\n",
      "[663]\ttraining's auc: 0.999157\tvalid_1's auc: 0.914259\n",
      "[664]\ttraining's auc: 0.99916\tvalid_1's auc: 0.914331\n",
      "[665]\ttraining's auc: 0.999163\tvalid_1's auc: 0.914409\n",
      "[666]\ttraining's auc: 0.999166\tvalid_1's auc: 0.914531\n",
      "[667]\ttraining's auc: 0.999168\tvalid_1's auc: 0.914544\n",
      "[668]\ttraining's auc: 0.999171\tvalid_1's auc: 0.914638\n",
      "[669]\ttraining's auc: 0.999174\tvalid_1's auc: 0.914684\n",
      "[670]\ttraining's auc: 0.999175\tvalid_1's auc: 0.914749\n",
      "[671]\ttraining's auc: 0.999178\tvalid_1's auc: 0.914805\n",
      "[672]\ttraining's auc: 0.99918\tvalid_1's auc: 0.914824\n",
      "[673]\ttraining's auc: 0.999182\tvalid_1's auc: 0.914851\n",
      "[674]\ttraining's auc: 0.999184\tvalid_1's auc: 0.914912\n",
      "[675]\ttraining's auc: 0.999187\tvalid_1's auc: 0.914996\n",
      "[676]\ttraining's auc: 0.999189\tvalid_1's auc: 0.915095\n",
      "[677]\ttraining's auc: 0.999193\tvalid_1's auc: 0.915145\n",
      "[678]\ttraining's auc: 0.999195\tvalid_1's auc: 0.915333\n",
      "[679]\ttraining's auc: 0.999199\tvalid_1's auc: 0.915461\n",
      "[680]\ttraining's auc: 0.999202\tvalid_1's auc: 0.915592\n",
      "[681]\ttraining's auc: 0.999204\tvalid_1's auc: 0.915628\n",
      "[682]\ttraining's auc: 0.999207\tvalid_1's auc: 0.915711\n",
      "[683]\ttraining's auc: 0.999209\tvalid_1's auc: 0.915775\n",
      "[684]\ttraining's auc: 0.999213\tvalid_1's auc: 0.915858\n",
      "[685]\ttraining's auc: 0.999215\tvalid_1's auc: 0.915982\n",
      "[686]\ttraining's auc: 0.999217\tvalid_1's auc: 0.916037\n",
      "[687]\ttraining's auc: 0.99922\tvalid_1's auc: 0.916144\n",
      "[688]\ttraining's auc: 0.999223\tvalid_1's auc: 0.916256\n",
      "[689]\ttraining's auc: 0.999226\tvalid_1's auc: 0.91634\n",
      "[690]\ttraining's auc: 0.999228\tvalid_1's auc: 0.916423\n",
      "[691]\ttraining's auc: 0.999231\tvalid_1's auc: 0.916575\n",
      "[692]\ttraining's auc: 0.999234\tvalid_1's auc: 0.916603\n",
      "[693]\ttraining's auc: 0.999235\tvalid_1's auc: 0.916635\n",
      "[694]\ttraining's auc: 0.999238\tvalid_1's auc: 0.916686\n",
      "[695]\ttraining's auc: 0.99924\tvalid_1's auc: 0.916716\n",
      "[696]\ttraining's auc: 0.999243\tvalid_1's auc: 0.916756\n",
      "[697]\ttraining's auc: 0.999245\tvalid_1's auc: 0.916809\n",
      "[698]\ttraining's auc: 0.999248\tvalid_1's auc: 0.916939\n",
      "[699]\ttraining's auc: 0.99925\tvalid_1's auc: 0.916942\n",
      "[700]\ttraining's auc: 0.999253\tvalid_1's auc: 0.91702\n",
      "[701]\ttraining's auc: 0.999255\tvalid_1's auc: 0.917116\n",
      "[702]\ttraining's auc: 0.999258\tvalid_1's auc: 0.917226\n",
      "[703]\ttraining's auc: 0.999261\tvalid_1's auc: 0.917359\n",
      "[704]\ttraining's auc: 0.999264\tvalid_1's auc: 0.91746\n",
      "[705]\ttraining's auc: 0.999266\tvalid_1's auc: 0.917547\n",
      "[706]\ttraining's auc: 0.999269\tvalid_1's auc: 0.917646\n",
      "[707]\ttraining's auc: 0.999271\tvalid_1's auc: 0.917779\n",
      "[708]\ttraining's auc: 0.999274\tvalid_1's auc: 0.917864\n",
      "[709]\ttraining's auc: 0.999277\tvalid_1's auc: 0.9179\n",
      "[710]\ttraining's auc: 0.999278\tvalid_1's auc: 0.91795\n",
      "[711]\ttraining's auc: 0.999281\tvalid_1's auc: 0.91803\n",
      "[712]\ttraining's auc: 0.999284\tvalid_1's auc: 0.918097\n",
      "[713]\ttraining's auc: 0.999286\tvalid_1's auc: 0.918135\n",
      "[714]\ttraining's auc: 0.999289\tvalid_1's auc: 0.918277\n",
      "[715]\ttraining's auc: 0.999292\tvalid_1's auc: 0.918368\n",
      "[716]\ttraining's auc: 0.999294\tvalid_1's auc: 0.918451\n",
      "[717]\ttraining's auc: 0.999296\tvalid_1's auc: 0.918509\n",
      "[718]\ttraining's auc: 0.999299\tvalid_1's auc: 0.918573\n",
      "[719]\ttraining's auc: 0.999301\tvalid_1's auc: 0.918632\n",
      "[720]\ttraining's auc: 0.999304\tvalid_1's auc: 0.918678\n",
      "[721]\ttraining's auc: 0.999306\tvalid_1's auc: 0.918817\n",
      "[722]\ttraining's auc: 0.999309\tvalid_1's auc: 0.918922\n",
      "[723]\ttraining's auc: 0.999312\tvalid_1's auc: 0.919014\n",
      "[724]\ttraining's auc: 0.999314\tvalid_1's auc: 0.919033\n",
      "[725]\ttraining's auc: 0.999316\tvalid_1's auc: 0.919158\n",
      "[726]\ttraining's auc: 0.99932\tvalid_1's auc: 0.919184\n",
      "[727]\ttraining's auc: 0.999322\tvalid_1's auc: 0.919276\n",
      "[728]\ttraining's auc: 0.999324\tvalid_1's auc: 0.919285\n",
      "[729]\ttraining's auc: 0.999327\tvalid_1's auc: 0.91934\n",
      "[730]\ttraining's auc: 0.999329\tvalid_1's auc: 0.919404\n",
      "[731]\ttraining's auc: 0.999332\tvalid_1's auc: 0.919487\n",
      "[732]\ttraining's auc: 0.999335\tvalid_1's auc: 0.919587\n",
      "[733]\ttraining's auc: 0.999338\tvalid_1's auc: 0.919709\n",
      "[734]\ttraining's auc: 0.99934\tvalid_1's auc: 0.919759\n",
      "[735]\ttraining's auc: 0.999342\tvalid_1's auc: 0.919789\n",
      "[736]\ttraining's auc: 0.999344\tvalid_1's auc: 0.919857\n",
      "[737]\ttraining's auc: 0.999346\tvalid_1's auc: 0.919981\n",
      "[738]\ttraining's auc: 0.999349\tvalid_1's auc: 0.920056\n",
      "[739]\ttraining's auc: 0.999351\tvalid_1's auc: 0.920181\n",
      "[740]\ttraining's auc: 0.999354\tvalid_1's auc: 0.920317\n",
      "[741]\ttraining's auc: 0.999356\tvalid_1's auc: 0.920404\n",
      "[742]\ttraining's auc: 0.999358\tvalid_1's auc: 0.920519\n",
      "[743]\ttraining's auc: 0.99936\tvalid_1's auc: 0.920551\n",
      "[744]\ttraining's auc: 0.999363\tvalid_1's auc: 0.920671\n",
      "[745]\ttraining's auc: 0.999366\tvalid_1's auc: 0.92086\n",
      "[746]\ttraining's auc: 0.999369\tvalid_1's auc: 0.920966\n",
      "[747]\ttraining's auc: 0.999371\tvalid_1's auc: 0.921053\n",
      "[748]\ttraining's auc: 0.999373\tvalid_1's auc: 0.921144\n",
      "[749]\ttraining's auc: 0.999376\tvalid_1's auc: 0.921201\n",
      "[750]\ttraining's auc: 0.999377\tvalid_1's auc: 0.921225\n",
      "[751]\ttraining's auc: 0.99938\tvalid_1's auc: 0.921283\n",
      "[752]\ttraining's auc: 0.999383\tvalid_1's auc: 0.921345\n",
      "[753]\ttraining's auc: 0.999384\tvalid_1's auc: 0.921386\n",
      "[754]\ttraining's auc: 0.999387\tvalid_1's auc: 0.921511\n",
      "[755]\ttraining's auc: 0.99939\tvalid_1's auc: 0.921575\n",
      "[756]\ttraining's auc: 0.999392\tvalid_1's auc: 0.921595\n",
      "[757]\ttraining's auc: 0.999394\tvalid_1's auc: 0.921654\n",
      "[758]\ttraining's auc: 0.999397\tvalid_1's auc: 0.921708\n",
      "[759]\ttraining's auc: 0.999399\tvalid_1's auc: 0.921817\n",
      "[760]\ttraining's auc: 0.999402\tvalid_1's auc: 0.921889\n",
      "[761]\ttraining's auc: 0.999404\tvalid_1's auc: 0.921923\n",
      "[762]\ttraining's auc: 0.999407\tvalid_1's auc: 0.922006\n",
      "[763]\ttraining's auc: 0.99941\tvalid_1's auc: 0.922131\n",
      "[764]\ttraining's auc: 0.999412\tvalid_1's auc: 0.92221\n",
      "[765]\ttraining's auc: 0.999414\tvalid_1's auc: 0.922268\n",
      "[766]\ttraining's auc: 0.999417\tvalid_1's auc: 0.922315\n",
      "[767]\ttraining's auc: 0.999418\tvalid_1's auc: 0.922348\n",
      "[768]\ttraining's auc: 0.99942\tvalid_1's auc: 0.922408\n",
      "[769]\ttraining's auc: 0.999423\tvalid_1's auc: 0.922511\n",
      "[770]\ttraining's auc: 0.999425\tvalid_1's auc: 0.922552\n",
      "[771]\ttraining's auc: 0.999428\tvalid_1's auc: 0.92266\n",
      "[772]\ttraining's auc: 0.99943\tvalid_1's auc: 0.922803\n",
      "[773]\ttraining's auc: 0.999432\tvalid_1's auc: 0.922919\n",
      "[774]\ttraining's auc: 0.999433\tvalid_1's auc: 0.92296\n",
      "[775]\ttraining's auc: 0.999436\tvalid_1's auc: 0.92304\n",
      "[776]\ttraining's auc: 0.999438\tvalid_1's auc: 0.923167\n",
      "[777]\ttraining's auc: 0.99944\tvalid_1's auc: 0.923233\n",
      "[778]\ttraining's auc: 0.999442\tvalid_1's auc: 0.923318\n",
      "[779]\ttraining's auc: 0.999444\tvalid_1's auc: 0.923433\n",
      "[780]\ttraining's auc: 0.999446\tvalid_1's auc: 0.923454\n",
      "[781]\ttraining's auc: 0.999448\tvalid_1's auc: 0.923525\n",
      "[782]\ttraining's auc: 0.999451\tvalid_1's auc: 0.923594\n",
      "[783]\ttraining's auc: 0.999453\tvalid_1's auc: 0.923617\n",
      "[784]\ttraining's auc: 0.999455\tvalid_1's auc: 0.923686\n",
      "[785]\ttraining's auc: 0.999457\tvalid_1's auc: 0.923697\n",
      "[786]\ttraining's auc: 0.99946\tvalid_1's auc: 0.923737\n",
      "[787]\ttraining's auc: 0.999462\tvalid_1's auc: 0.923793\n",
      "[788]\ttraining's auc: 0.999464\tvalid_1's auc: 0.923846\n",
      "[789]\ttraining's auc: 0.999466\tvalid_1's auc: 0.923884\n",
      "[790]\ttraining's auc: 0.999468\tvalid_1's auc: 0.924013\n",
      "[791]\ttraining's auc: 0.999471\tvalid_1's auc: 0.924064\n",
      "[792]\ttraining's auc: 0.999473\tvalid_1's auc: 0.924183\n",
      "[793]\ttraining's auc: 0.999476\tvalid_1's auc: 0.924297\n",
      "[794]\ttraining's auc: 0.999478\tvalid_1's auc: 0.92442\n",
      "[795]\ttraining's auc: 0.99948\tvalid_1's auc: 0.924444\n",
      "[796]\ttraining's auc: 0.999482\tvalid_1's auc: 0.924542\n",
      "[797]\ttraining's auc: 0.999483\tvalid_1's auc: 0.924622\n",
      "[798]\ttraining's auc: 0.999485\tvalid_1's auc: 0.924627\n",
      "[799]\ttraining's auc: 0.999487\tvalid_1's auc: 0.924689\n",
      "[800]\ttraining's auc: 0.999489\tvalid_1's auc: 0.924801\n",
      "[801]\ttraining's auc: 0.999491\tvalid_1's auc: 0.924869\n",
      "[802]\ttraining's auc: 0.999493\tvalid_1's auc: 0.924961\n",
      "[803]\ttraining's auc: 0.999496\tvalid_1's auc: 0.925021\n",
      "[804]\ttraining's auc: 0.999498\tvalid_1's auc: 0.925153\n",
      "[805]\ttraining's auc: 0.999501\tvalid_1's auc: 0.925249\n",
      "[806]\ttraining's auc: 0.999503\tvalid_1's auc: 0.925308\n",
      "[807]\ttraining's auc: 0.999505\tvalid_1's auc: 0.92539\n",
      "[808]\ttraining's auc: 0.999507\tvalid_1's auc: 0.925437\n",
      "[809]\ttraining's auc: 0.999509\tvalid_1's auc: 0.925549\n",
      "[810]\ttraining's auc: 0.999512\tvalid_1's auc: 0.925576\n",
      "[811]\ttraining's auc: 0.999514\tvalid_1's auc: 0.925637\n",
      "[812]\ttraining's auc: 0.999516\tvalid_1's auc: 0.925705\n",
      "[813]\ttraining's auc: 0.999518\tvalid_1's auc: 0.925806\n",
      "[814]\ttraining's auc: 0.999521\tvalid_1's auc: 0.925884\n",
      "[815]\ttraining's auc: 0.999523\tvalid_1's auc: 0.925957\n",
      "[816]\ttraining's auc: 0.999525\tvalid_1's auc: 0.926048\n",
      "[817]\ttraining's auc: 0.999527\tvalid_1's auc: 0.926073\n",
      "[818]\ttraining's auc: 0.999529\tvalid_1's auc: 0.926088\n",
      "[819]\ttraining's auc: 0.999531\tvalid_1's auc: 0.926165\n",
      "[820]\ttraining's auc: 0.999533\tvalid_1's auc: 0.926201\n",
      "[821]\ttraining's auc: 0.999535\tvalid_1's auc: 0.926224\n",
      "[822]\ttraining's auc: 0.999537\tvalid_1's auc: 0.926277\n",
      "[823]\ttraining's auc: 0.999539\tvalid_1's auc: 0.926392\n",
      "[824]\ttraining's auc: 0.999541\tvalid_1's auc: 0.926428\n",
      "[825]\ttraining's auc: 0.999543\tvalid_1's auc: 0.926464\n",
      "[826]\ttraining's auc: 0.999544\tvalid_1's auc: 0.926528\n",
      "[827]\ttraining's auc: 0.999547\tvalid_1's auc: 0.926648\n",
      "[828]\ttraining's auc: 0.999549\tvalid_1's auc: 0.926686\n",
      "[829]\ttraining's auc: 0.999551\tvalid_1's auc: 0.926714\n",
      "[830]\ttraining's auc: 0.999552\tvalid_1's auc: 0.926813\n",
      "[831]\ttraining's auc: 0.999554\tvalid_1's auc: 0.926841\n",
      "[832]\ttraining's auc: 0.999557\tvalid_1's auc: 0.926956\n",
      "[833]\ttraining's auc: 0.999559\tvalid_1's auc: 0.927123\n",
      "[834]\ttraining's auc: 0.999561\tvalid_1's auc: 0.927166\n",
      "[835]\ttraining's auc: 0.999562\tvalid_1's auc: 0.927199\n",
      "[836]\ttraining's auc: 0.999565\tvalid_1's auc: 0.927223\n",
      "[837]\ttraining's auc: 0.999566\tvalid_1's auc: 0.92727\n",
      "[838]\ttraining's auc: 0.999568\tvalid_1's auc: 0.927329\n",
      "[839]\ttraining's auc: 0.999569\tvalid_1's auc: 0.927376\n",
      "[840]\ttraining's auc: 0.999571\tvalid_1's auc: 0.927407\n",
      "[841]\ttraining's auc: 0.999573\tvalid_1's auc: 0.927518\n",
      "[842]\ttraining's auc: 0.999576\tvalid_1's auc: 0.92761\n",
      "[843]\ttraining's auc: 0.999578\tvalid_1's auc: 0.927647\n",
      "[844]\ttraining's auc: 0.99958\tvalid_1's auc: 0.927751\n",
      "[845]\ttraining's auc: 0.999582\tvalid_1's auc: 0.927821\n",
      "[846]\ttraining's auc: 0.999584\tvalid_1's auc: 0.927864\n",
      "[847]\ttraining's auc: 0.999586\tvalid_1's auc: 0.927955\n",
      "[848]\ttraining's auc: 0.999588\tvalid_1's auc: 0.927996\n",
      "[849]\ttraining's auc: 0.99959\tvalid_1's auc: 0.928016\n",
      "[850]\ttraining's auc: 0.999592\tvalid_1's auc: 0.928018\n",
      "[851]\ttraining's auc: 0.999593\tvalid_1's auc: 0.928047\n",
      "[852]\ttraining's auc: 0.999595\tvalid_1's auc: 0.928165\n",
      "[853]\ttraining's auc: 0.999597\tvalid_1's auc: 0.928187\n",
      "[854]\ttraining's auc: 0.999599\tvalid_1's auc: 0.928227\n",
      "[855]\ttraining's auc: 0.999601\tvalid_1's auc: 0.92825\n",
      "[856]\ttraining's auc: 0.999602\tvalid_1's auc: 0.928286\n",
      "[857]\ttraining's auc: 0.999604\tvalid_1's auc: 0.928324\n",
      "[858]\ttraining's auc: 0.999606\tvalid_1's auc: 0.928354\n",
      "[859]\ttraining's auc: 0.999607\tvalid_1's auc: 0.928363\n",
      "[860]\ttraining's auc: 0.999609\tvalid_1's auc: 0.928396\n",
      "[861]\ttraining's auc: 0.999611\tvalid_1's auc: 0.928541\n",
      "[862]\ttraining's auc: 0.999613\tvalid_1's auc: 0.928572\n",
      "[863]\ttraining's auc: 0.999614\tvalid_1's auc: 0.92861\n",
      "[864]\ttraining's auc: 0.999616\tvalid_1's auc: 0.928693\n",
      "[865]\ttraining's auc: 0.999618\tvalid_1's auc: 0.928807\n",
      "[866]\ttraining's auc: 0.99962\tvalid_1's auc: 0.928869\n",
      "[867]\ttraining's auc: 0.999623\tvalid_1's auc: 0.928983\n",
      "[868]\ttraining's auc: 0.999624\tvalid_1's auc: 0.929063\n",
      "[869]\ttraining's auc: 0.999625\tvalid_1's auc: 0.929158\n",
      "[870]\ttraining's auc: 0.999627\tvalid_1's auc: 0.929162\n",
      "[871]\ttraining's auc: 0.999629\tvalid_1's auc: 0.929283\n",
      "[872]\ttraining's auc: 0.999631\tvalid_1's auc: 0.929377\n",
      "[873]\ttraining's auc: 0.999632\tvalid_1's auc: 0.929395\n",
      "[874]\ttraining's auc: 0.999634\tvalid_1's auc: 0.929425\n",
      "[875]\ttraining's auc: 0.999636\tvalid_1's auc: 0.92951\n",
      "[876]\ttraining's auc: 0.999637\tvalid_1's auc: 0.929512\n",
      "[877]\ttraining's auc: 0.999639\tvalid_1's auc: 0.929546\n",
      "[878]\ttraining's auc: 0.999641\tvalid_1's auc: 0.929568\n",
      "[879]\ttraining's auc: 0.999642\tvalid_1's auc: 0.929655\n",
      "[880]\ttraining's auc: 0.999644\tvalid_1's auc: 0.929727\n",
      "[881]\ttraining's auc: 0.999646\tvalid_1's auc: 0.929849\n",
      "[882]\ttraining's auc: 0.999647\tvalid_1's auc: 0.929945\n",
      "[883]\ttraining's auc: 0.999649\tvalid_1's auc: 0.929972\n",
      "[884]\ttraining's auc: 0.999651\tvalid_1's auc: 0.93001\n",
      "[885]\ttraining's auc: 0.999652\tvalid_1's auc: 0.930097\n",
      "[886]\ttraining's auc: 0.999654\tvalid_1's auc: 0.93016\n",
      "[887]\ttraining's auc: 0.999656\tvalid_1's auc: 0.930226\n",
      "[888]\ttraining's auc: 0.999657\tvalid_1's auc: 0.930298\n",
      "[889]\ttraining's auc: 0.999659\tvalid_1's auc: 0.930413\n",
      "[890]\ttraining's auc: 0.999661\tvalid_1's auc: 0.930407\n",
      "[891]\ttraining's auc: 0.999662\tvalid_1's auc: 0.93046\n",
      "[892]\ttraining's auc: 0.999664\tvalid_1's auc: 0.930483\n",
      "[893]\ttraining's auc: 0.999665\tvalid_1's auc: 0.930531\n",
      "[894]\ttraining's auc: 0.999667\tvalid_1's auc: 0.930611\n",
      "[895]\ttraining's auc: 0.999668\tvalid_1's auc: 0.930633\n",
      "[896]\ttraining's auc: 0.99967\tvalid_1's auc: 0.930693\n",
      "[897]\ttraining's auc: 0.999672\tvalid_1's auc: 0.930719\n",
      "[898]\ttraining's auc: 0.999673\tvalid_1's auc: 0.93081\n",
      "[899]\ttraining's auc: 0.999675\tvalid_1's auc: 0.930861\n",
      "[900]\ttraining's auc: 0.999677\tvalid_1's auc: 0.930908\n",
      "[901]\ttraining's auc: 0.999679\tvalid_1's auc: 0.930995\n",
      "[902]\ttraining's auc: 0.999681\tvalid_1's auc: 0.931025\n",
      "[903]\ttraining's auc: 0.999682\tvalid_1's auc: 0.931105\n",
      "[904]\ttraining's auc: 0.999684\tvalid_1's auc: 0.931112\n",
      "[905]\ttraining's auc: 0.999685\tvalid_1's auc: 0.931143\n",
      "[906]\ttraining's auc: 0.999687\tvalid_1's auc: 0.931197\n",
      "[907]\ttraining's auc: 0.999689\tvalid_1's auc: 0.931244\n",
      "[908]\ttraining's auc: 0.99969\tvalid_1's auc: 0.931306\n",
      "[909]\ttraining's auc: 0.999692\tvalid_1's auc: 0.93133\n",
      "[910]\ttraining's auc: 0.999693\tvalid_1's auc: 0.931379\n",
      "[911]\ttraining's auc: 0.999695\tvalid_1's auc: 0.9314\n",
      "[912]\ttraining's auc: 0.999696\tvalid_1's auc: 0.931524\n",
      "[913]\ttraining's auc: 0.999698\tvalid_1's auc: 0.931615\n",
      "[914]\ttraining's auc: 0.9997\tvalid_1's auc: 0.93169\n",
      "[915]\ttraining's auc: 0.999701\tvalid_1's auc: 0.931741\n",
      "[916]\ttraining's auc: 0.999703\tvalid_1's auc: 0.931822\n",
      "[917]\ttraining's auc: 0.999705\tvalid_1's auc: 0.931861\n",
      "[918]\ttraining's auc: 0.999706\tvalid_1's auc: 0.93187\n",
      "[919]\ttraining's auc: 0.999708\tvalid_1's auc: 0.931907\n",
      "[920]\ttraining's auc: 0.999709\tvalid_1's auc: 0.932039\n",
      "[921]\ttraining's auc: 0.999711\tvalid_1's auc: 0.932159\n",
      "[922]\ttraining's auc: 0.999712\tvalid_1's auc: 0.932182\n",
      "[923]\ttraining's auc: 0.999713\tvalid_1's auc: 0.932291\n",
      "[924]\ttraining's auc: 0.999714\tvalid_1's auc: 0.932355\n",
      "[925]\ttraining's auc: 0.999716\tvalid_1's auc: 0.9324\n",
      "[926]\ttraining's auc: 0.999717\tvalid_1's auc: 0.932487\n",
      "[927]\ttraining's auc: 0.999718\tvalid_1's auc: 0.932533\n",
      "[928]\ttraining's auc: 0.999719\tvalid_1's auc: 0.932559\n",
      "[929]\ttraining's auc: 0.999721\tvalid_1's auc: 0.932559\n",
      "[930]\ttraining's auc: 0.999722\tvalid_1's auc: 0.932589\n",
      "[931]\ttraining's auc: 0.999723\tvalid_1's auc: 0.932593\n",
      "[932]\ttraining's auc: 0.999725\tvalid_1's auc: 0.932746\n",
      "[933]\ttraining's auc: 0.999726\tvalid_1's auc: 0.932779\n",
      "[934]\ttraining's auc: 0.999728\tvalid_1's auc: 0.932851\n",
      "[935]\ttraining's auc: 0.999729\tvalid_1's auc: 0.932919\n",
      "[936]\ttraining's auc: 0.999731\tvalid_1's auc: 0.932976\n",
      "[937]\ttraining's auc: 0.999732\tvalid_1's auc: 0.933025\n",
      "[938]\ttraining's auc: 0.999733\tvalid_1's auc: 0.933041\n",
      "[939]\ttraining's auc: 0.999735\tvalid_1's auc: 0.933121\n",
      "[940]\ttraining's auc: 0.999736\tvalid_1's auc: 0.933136\n",
      "[941]\ttraining's auc: 0.999738\tvalid_1's auc: 0.933233\n",
      "[942]\ttraining's auc: 0.999739\tvalid_1's auc: 0.933363\n",
      "[943]\ttraining's auc: 0.99974\tvalid_1's auc: 0.93341\n",
      "[944]\ttraining's auc: 0.999742\tvalid_1's auc: 0.933462\n",
      "[945]\ttraining's auc: 0.999743\tvalid_1's auc: 0.933507\n",
      "[946]\ttraining's auc: 0.999744\tvalid_1's auc: 0.933555\n",
      "[947]\ttraining's auc: 0.999745\tvalid_1's auc: 0.933543\n",
      "[948]\ttraining's auc: 0.999746\tvalid_1's auc: 0.933624\n",
      "[949]\ttraining's auc: 0.999748\tvalid_1's auc: 0.933714\n",
      "[950]\ttraining's auc: 0.999749\tvalid_1's auc: 0.933894\n",
      "[951]\ttraining's auc: 0.999751\tvalid_1's auc: 0.933985\n",
      "[952]\ttraining's auc: 0.999752\tvalid_1's auc: 0.934073\n",
      "[953]\ttraining's auc: 0.999754\tvalid_1's auc: 0.934144\n",
      "[954]\ttraining's auc: 0.999755\tvalid_1's auc: 0.934203\n",
      "[955]\ttraining's auc: 0.999756\tvalid_1's auc: 0.934243\n",
      "[956]\ttraining's auc: 0.999758\tvalid_1's auc: 0.934305\n",
      "[957]\ttraining's auc: 0.999759\tvalid_1's auc: 0.934363\n",
      "[958]\ttraining's auc: 0.99976\tvalid_1's auc: 0.934401\n",
      "[959]\ttraining's auc: 0.999762\tvalid_1's auc: 0.934484\n",
      "[960]\ttraining's auc: 0.999763\tvalid_1's auc: 0.934519\n",
      "[961]\ttraining's auc: 0.999764\tvalid_1's auc: 0.934605\n",
      "[962]\ttraining's auc: 0.999765\tvalid_1's auc: 0.934658\n",
      "[963]\ttraining's auc: 0.999766\tvalid_1's auc: 0.934708\n",
      "[964]\ttraining's auc: 0.999767\tvalid_1's auc: 0.934764\n",
      "[965]\ttraining's auc: 0.999768\tvalid_1's auc: 0.934807\n",
      "[966]\ttraining's auc: 0.999769\tvalid_1's auc: 0.93484\n",
      "[967]\ttraining's auc: 0.99977\tvalid_1's auc: 0.934874\n",
      "[968]\ttraining's auc: 0.999772\tvalid_1's auc: 0.934881\n",
      "[969]\ttraining's auc: 0.999773\tvalid_1's auc: 0.934922\n",
      "[970]\ttraining's auc: 0.999774\tvalid_1's auc: 0.934951\n",
      "[971]\ttraining's auc: 0.999776\tvalid_1's auc: 0.934991\n",
      "[972]\ttraining's auc: 0.999777\tvalid_1's auc: 0.934981\n",
      "[973]\ttraining's auc: 0.999778\tvalid_1's auc: 0.935072\n",
      "[974]\ttraining's auc: 0.999779\tvalid_1's auc: 0.935117\n",
      "[975]\ttraining's auc: 0.99978\tvalid_1's auc: 0.935199\n",
      "[976]\ttraining's auc: 0.999781\tvalid_1's auc: 0.935256\n",
      "[977]\ttraining's auc: 0.999783\tvalid_1's auc: 0.935327\n",
      "[978]\ttraining's auc: 0.999784\tvalid_1's auc: 0.935381\n",
      "[979]\ttraining's auc: 0.999785\tvalid_1's auc: 0.935461\n",
      "[980]\ttraining's auc: 0.999786\tvalid_1's auc: 0.935509\n",
      "[981]\ttraining's auc: 0.999788\tvalid_1's auc: 0.93556\n",
      "[982]\ttraining's auc: 0.999789\tvalid_1's auc: 0.935631\n",
      "[983]\ttraining's auc: 0.99979\tvalid_1's auc: 0.935673\n",
      "[984]\ttraining's auc: 0.999791\tvalid_1's auc: 0.935669\n",
      "[985]\ttraining's auc: 0.999792\tvalid_1's auc: 0.935709\n",
      "[986]\ttraining's auc: 0.999793\tvalid_1's auc: 0.935744\n",
      "[987]\ttraining's auc: 0.999794\tvalid_1's auc: 0.935752\n",
      "[988]\ttraining's auc: 0.999795\tvalid_1's auc: 0.935787\n",
      "[989]\ttraining's auc: 0.999796\tvalid_1's auc: 0.935819\n",
      "[990]\ttraining's auc: 0.999797\tvalid_1's auc: 0.935898\n",
      "[991]\ttraining's auc: 0.999798\tvalid_1's auc: 0.935926\n",
      "[992]\ttraining's auc: 0.999799\tvalid_1's auc: 0.935964\n",
      "[993]\ttraining's auc: 0.9998\tvalid_1's auc: 0.936042\n",
      "[994]\ttraining's auc: 0.999801\tvalid_1's auc: 0.936089\n",
      "[995]\ttraining's auc: 0.999802\tvalid_1's auc: 0.936126\n",
      "[996]\ttraining's auc: 0.999804\tvalid_1's auc: 0.93619\n",
      "[997]\ttraining's auc: 0.999805\tvalid_1's auc: 0.936203\n",
      "[998]\ttraining's auc: 0.999806\tvalid_1's auc: 0.936251\n",
      "[999]\ttraining's auc: 0.999807\tvalid_1's auc: 0.936277\n",
      "[1000]\ttraining's auc: 0.999808\tvalid_1's auc: 0.936292\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "params = {'num_leaves': 491,\n",
    "          'min_child_weight': 0.03454472573214212,\n",
    "          'feature_fraction': 0.3797454081646243,\n",
    "          'bagging_fraction': 0.4181193142567742,\n",
    "          'min_data_in_leaf': 106,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.006883242363721497,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'auc',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.3899927210061127,\n",
    "          'reg_lambda': 0.6485237330340494,\n",
    "          'random_state': 5313,\n",
    "         }\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dvalid = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "lgb_clf = lgb.train(params, dtrain, 1000, valid_sets = [dtrain, dvalid])\n",
    "\n",
    "\n",
    "feature_importances = lgb_clf.feature_importance()\n",
    "\n",
    "y_pred_valid = lgb_clf.predict(X_test)\n",
    "score = roc_auc_score(y_test, y_pred_valid)\n",
    "print(f\"AUC = {score}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adaboost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9679679976294541\n"
     ]
    }
   ],
   "source": [
    "#Import required modules\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Build the model\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "adaModel = clf.fit(X_train, y_train)\n",
    "\n",
    "#Adaboost Score\n",
    "adaScore = adaModel.score(X_test, y_test)\n",
    "print(adaScore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Ensemble\n",
    "\n",
    "Having fitted several models, we now combine them to try squeeze out any further predictive performance.\n",
    "This is achieved through the use of SKLearn's StackingClassifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.model_stack import stack_models\n",
    "\n",
    "models = {'xgb':xgb_cl,\n",
    "          'bag': bag_clf,\n",
    "          'lgb': lgb_clf,\n",
    "          'rf':rf_final,\n",
    "          'AdaBoost': adaModel\n",
    "          }\n",
    "stacked_model = stack_models(*models.items(), cv_folds=3)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "models['lgbm'] = lgb_clf\n",
    "models['stacked'] = stacked_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataframe to show the performance of each model\n",
    "performance = pd.DataFrame(y_test)\n",
    "performance['stacking_prediction'] = stacked_model.predict(X_test)\n",
    "for m in stacked_model.named_estimators_:\n",
    "        performance[m] = stacked_model.named_estimators_[m].predict(X_test)\n",
    "\n",
    "\n",
    "# See how each of our models correlate with our target\n",
    "print(\"Correlations with target column\")\n",
    "print(performance.corr()['isFraud'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model evaluation using stratified folds\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "# evaluate models using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\treturn scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, m in models.items():\n",
    "\tscores = evaluate_model(m, X_test, y_test)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint(f'>{name} {mean(scores):0.3f} ({std(scores):0.3f})')\n",
    "\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.diagnostics import get_diagnostics\n",
    "get_diagnostics(*models.items(), y_true=y_test, X=X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}